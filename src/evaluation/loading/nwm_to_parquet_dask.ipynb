{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877757a0-69be-4c7c-b1fd-6e4460ff7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# adding project dirs to path so code may be referenced from the notebook\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51601028-c397-4070-a33d-d47f7dc21ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import config\n",
    "import utils\n",
    "import importlib\n",
    "import dask\n",
    "import xarray as xr\n",
    "from typing import Iterable, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import importlib\n",
    "import grid_to_parquet\n",
    "importlib.reload(grid_to_parquet)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from rasterio.io import MemoryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d61d956b-e4ca-426a-98a8-9493d32fce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs = utils.get_usgs_gages()\n",
    "nwm_feature_id_filter = usgs[\"nwm_feature_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb517ab-b2b3-457c-a050-f69c2fcec863",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=4, threads_per_worker=4)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94684a54-eba2-4729-af79-7ddd82b8aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "        blob_name: str,\n",
    "        use_cache: bool = True,\n",
    "        nwm_feature_id_filter: Union[Iterable, None] = None\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Retrieve a blob from the data service as xarray.Dataset.\n",
    "\n",
    "    Based largely on OWP HydroTools.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    blob_name: str, required\n",
    "        Name of blob to retrieve.\n",
    "    use_cacahe: bool, default True\n",
    "        If cache should be used.  \n",
    "        If True, checks to see if file is in cache, and \n",
    "        if fetched from remote will save to cache.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : xarray.Dataset\n",
    "        The data stored in the blob.\n",
    "\n",
    "    \"\"\"\n",
    "    nc_filepath = os.path.join(utils.get_cache_dir(), blob_name)\n",
    "    utils.make_parent_dir(nc_filepath)\n",
    "\n",
    "    # If the file exists and use_cache = True\n",
    "    if os.path.exists(nc_filepath) and use_cache:\n",
    "        # Get dataset from cache\n",
    "        ds = xr.load_dataset(\n",
    "            nc_filepath,\n",
    "            engine='h5netcdf',\n",
    "        )\n",
    "        return ds\n",
    "    else:\n",
    "        # Get raw bytes\n",
    "        raw_bytes = grid_to_parquet.get_blob(blob_name)\n",
    "        # Create Dataset\n",
    "        ds = xr.load_dataset(\n",
    "            MemoryFile(raw_bytes),\n",
    "            engine='h5netcdf',\n",
    "        )\n",
    "        if use_cache:\n",
    "            # Subset and cache\n",
    "            ds.to_netcdf(\n",
    "                nc_filepath,\n",
    "                engine='h5netcdf',\n",
    "            )\n",
    "        return ds\n",
    "    \n",
    "    if nwm_feature_id_filter and isinstance(nwm_feature_id_filter, Iterable):\n",
    "            try:\n",
    "                nwm_feature_id_filter = list(nwm_feature_id_filter)\n",
    "                return ds.sel(feature_id=nwm_feature_id_filter)\n",
    "            except:\n",
    "                warnings.warn(\"Invalid feature_id_filter\")\n",
    "                return ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138abe71-8123-4352-b199-55e90936ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nwm_to_parquet(blob_name, use_cache: bool = False):\n",
    "    \"\"\"Calculate the MAP for a single NetCDF file (i.e. one timestep).\n",
    "\n",
    "    ToDo: add way to filter which catchments are calculated\n",
    "    \"\"\"\n",
    "    # print(f\"Processing {blob_name}, {datetime.now()}\")\n",
    "\n",
    "    # Get some metainfo from blob_name\n",
    "    path_split = blob_name.split(\"/\")\n",
    "    reference_time = datetime.strptime(\n",
    "        path_split[0].split(\".\")[1] + path_split[2].split(\".\")[1],\n",
    "        \"%Y%m%dt%Hz\"\n",
    "    )\n",
    "    offset_hours = int(path_split[2].split(\".\")[4][1:])  # f001\n",
    "    value_time = reference_time + timedelta(hours=offset_hours)\n",
    "    configuration = path_split[1]\n",
    "\n",
    "    # Get xr.Dataset/xr.DataArray\n",
    "    ds = get_dataset(blob_name, use_cache)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = ds[['reference_time', 'time', 'streamflow']].to_dataframe().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={\n",
    "        'time': 'value_time',\n",
    "        'feature_id': 'nwm_feature_id'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Sort values\n",
    "    df.sort_values(\n",
    "        by=['nwm_feature_id', 'value_time'],\n",
    "        ignore_index=True,\n",
    "        inplace=True\n",
    "    )\n",
    "        \n",
    "    # Additional columns\n",
    "    df['configuration'] = 'medium_range_mem1'\n",
    "    df['measurement_unit'] = 'm3/s'\n",
    "    df['variable_name'] = 'streamflow'\n",
    "\n",
    "    # Categorize\n",
    "    df['configuration'] = df['configuration'].astype(\"category\")\n",
    "    df['measurement_unit'] = df['measurement_unit'].astype(\"category\")\n",
    "    df['variable_name'] = df['variable_name'].astype(\"category\")\n",
    "        \n",
    "    # Save as parquet file\n",
    "    parquet_filepath = os.path.join(config.PARQUET_CACHE_DIR, f\"{blob_name}.parquet\")\n",
    "    utils.make_parent_dir(parquet_filepath)\n",
    "    df.to_parquet(parquet_filepath)\n",
    "    \n",
    "     # This should not be needed, but without memory usage grows\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()\n",
    "    \n",
    "    # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082cf3d-3a92-4854-b1f2-33ea8f3fb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some criteria\n",
    "ingest_days = 30\n",
    "start_dt = datetime(2022, 12, 18, 6) # First one is at 00Z in date\n",
    "td = timedelta(hours=6)\n",
    "number_of_forecasts = 3 #ingest_days * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981d25a-5670-4c88-bbcd-7c23c54e6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "# Loop though forecasts, fetch and insert\n",
    "for f in range(number_of_forecasts):\n",
    "    reference_time = start_dt + td * f\n",
    "    ref_time_str = reference_time.strftime(\"%Y%m%dT%HZ\")\n",
    "    configuration = \"medium_range_mem1\"\n",
    "\n",
    "    print(f\"Start download of {ref_time_str}\")\n",
    "\n",
    "    blob_list = grid_to_parquet.list_blobs_forcing(\n",
    "        configuration=configuration,\n",
    "        reference_time = ref_time_str,\n",
    "        must_contain = \"channel_rt\"\n",
    "    )\n",
    "\n",
    "    dfs = []\n",
    "    for blob_name in blob_list:\n",
    "        df = dask.delayed(nwm_to_parquet)(\n",
    "            blob_name, \n",
    "            use_cache=True, \n",
    "        )\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Join all timesteps into single pd.DataFrame\n",
    "    # results = dask.compute(*dfs)\n",
    "    dask.compute(*dfs)\n",
    "    \n",
    "#     # df = pd.concat(results)\n",
    "#     df = pd.concat(results)\n",
    "    \n",
    "#     # Sort values\n",
    "#     df.sort_values(\n",
    "#         by=['nwm_feature_id', 'value_time'],\n",
    "#         ignore_index=True,\n",
    "#         inplace=True\n",
    "#     )\n",
    "        \n",
    "#     # Additional columns\n",
    "#     df['configuration'] = configuration\n",
    "#     df['measurement_unit'] = 'm3/s'\n",
    "#     df['variable_name'] = 'streamflow'\n",
    "\n",
    "#     # Categorize\n",
    "#     df['configuration'] = df['configuration'].astype(\"category\")\n",
    "#     df['measurement_unit'] = df['measurement_unit'].astype(\"category\")\n",
    "#     df['variable_name'] = df['variable_name'].astype(\"category\")\n",
    "        \n",
    "#     # Save as parquet file\n",
    "#     parquet_filepath = os.path.join(config.MEDIUM_RANGE_1_PARQUET, f\"{ref_time_str}.parquet\")\n",
    "#     utils.make_parent_dir(parquet_filepath)\n",
    "#     df.to_parquet(parquet_filepath)\n",
    "    \n",
    "#     del df\n",
    "#     gc.collect()\n",
    "\n",
    "    # Print out some DataFrame stats\n",
    "    # print(df.info(verbose=True, memory_usage='deep'))\n",
    "    # print(df.memory_usage(index=True, deep=True))\n",
    "#     \n",
    "print(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
