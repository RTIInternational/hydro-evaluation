{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a88d62-8a46-4f48-a0ee-bd66bb3bad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fsspec\n",
    "import ujson\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import xarray as xr\n",
    "import dask\n",
    "import hvplot.xarray\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a89eb-5213-4a29-92ca-fe942c5f21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, progress\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba53e5c-a865-4599-aaac-74a447e61ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding project dirs to path so code may be referenced from the notebook\n",
    "import sys\n",
    "sys.path.insert(0, '../../evaluation')\n",
    "sys.path.insert(0, '../../evaluation/queries')\n",
    "sys.path.insert(0, '../../evaluation/loading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f801aa-28b1-4562-b398-b29f103a9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query some forcast data from parquet files\n",
    "import importlib\n",
    "import queries\n",
    "import config\n",
    "import utils as hu\n",
    "importlib.reload(queries)\n",
    "importlib.reload(config)\n",
    "importlib.reload(hu)\n",
    "import grid_to_parquet\n",
    "importlib.reload(grid_to_parquet)\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2d070-0896-4bdf-94e9-36940a6b9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup some criteria\n",
    "# ingest_days = 30\n",
    "# start_dt = datetime(2022, 12, 18, 6) # First one is at 00Z in date\n",
    "# td = timedelta(hours=6)\n",
    "# number_of_forecasts = 1 #ingest_days * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d29c19-5b5e-471e-8130-a3da5c44b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gcs', anon=True)\n",
    "fs2 = fsspec.filesystem('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34120b4-7e60-472c-8f02-4f91f557449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_dir = 'forcing_jsons/'\n",
    "\n",
    "# if not os.path.exists(json_dir):\n",
    "#     os.makedirs(json_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06442ae-6182-4f1c-8af1-f58555ac32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first') # args to fs.open()\n",
    "# default_fill_cache=False avoids caching data in between file chunks to lowers memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623b068-7efc-436c-9a36-fd4a9bd2a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_json(u):\n",
    "#     with fs.open(u, **so) as infile:\n",
    "#         h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "#         p = u.split('/')\n",
    "#         date = p[3]\n",
    "#         fname = p[5]\n",
    "#         outf = f'{json_dir}{date}.{fname}.json'\n",
    "#         with open(outf, 'wb') as f:\n",
    "#             f.write(ujson.dumps(h5chunks.translate()).encode());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443a30b-65d7-4932-95fb-5ef46464f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(datetime.now())\n",
    "# # Loop though forecasts, fetch and insert\n",
    "# for f in range(number_of_forecasts):\n",
    "#     reference_time = start_dt + td * f\n",
    "#     ref_time_str = reference_time.strftime(\"%Y%m%dT%HZ\")\n",
    "#     configuration = \"forcing_medium_range\"\n",
    "\n",
    "#     print(f\"Start download of {ref_time_str}\")\n",
    "\n",
    "#     blob_list = grid_to_parquet.list_blobs_forcing(\n",
    "#         configuration=configuration,\n",
    "#         reference_time = ref_time_str,\n",
    "#         must_contain = \"forcing\"\n",
    "#     )\n",
    "    \n",
    "#     blob_list = [f\"gcs://national-water-model/{b}\" for b in blob_list]\n",
    "    \n",
    "#     results = dask.compute(*[dask.delayed(gen_json)(u) for u in blob_list], retries=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b6c46-4e93-4506-a80a-7082c1c1ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_list = fs2.glob(f'{json_dir}/nwm.20221218.nwm.t06z*.json')\n",
    "# json_list = sorted(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb25c5-0db9-41f2-b9be-35ab3a4a174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mzz = MultiZarrToZarr(json_list,\n",
    "#         remote_protocol='gcs',\n",
    "#         remote_options={'anon':True},\n",
    "#         concat_dims=['time'],\n",
    "#         identical_dims = ['x', 'y'],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5bea4-a37c-428d-834b-104f9b5a4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# mzz.translate('nwm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c96003-e99a-4d10-90fe-7eb981f5472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend_args = { \"consolidated\": False,\n",
    "#                  \"storage_options\": { \"fo\": 'forcing_jsons/nwm.20221218.nwm.t06z.medium_range.forcing.f001.conus.nc.json',\n",
    "#                                 \"remote_protocol\": \"gcs\", \n",
    "#                                 \"remote_options\": {'anon':True} }}\n",
    "# ds = xr.open_dataset(\n",
    "#     \"reference://\", engine=\"zarr\",\n",
    "#     backend_kwargs=backend_args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9cab5-4788-4791-ab61-de9304c500d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# src = ds[\"RAINRATE\"].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfb40d-7bd1-4451-862e-2f806adacddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df = grid_to_parquet.calc_zonal_stats_weights(\n",
    "#     src=src,\n",
    "#     weights_filepath=config.HUC10_MEDIUM_RANGE_WEIGHTS_FILEPATH\n",
    "# )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467cbe-a192-49fc-88a3-b232921e5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json2(blob_in, json_out):\n",
    "    print(f\"gen_json: {json_out}\")\n",
    "    with fs.open(blob_in, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, blob_in, inline_threshold=300)\n",
    "        with open(json_out, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44546d5a-98ef-42f1-abb0-edd840385dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zonal_stats_weights(\n",
    "    src: xr.DataArray,\n",
    "    weights_filepath: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculates zonal stats\"\"\"\n",
    "\n",
    "    # Open weights dict from pickle\n",
    "    # This could probably be done once and passed as a reference.\n",
    "    with open(weights_filepath, 'rb') as f:\n",
    "        crosswalk_dict = pickle.load(f)\n",
    "\n",
    "    r_array = src.values[0]\n",
    "    r_array[r_array == src.rio.nodata] = np.nan\n",
    "\n",
    "    mean_dict = {}\n",
    "    for key, value in crosswalk_dict.items():\n",
    "        mean_dict[key] = np.nanmean(r_array[value])\n",
    "\n",
    "    df = pd.DataFrame.from_dict(mean_dict,\n",
    "                                orient='index',\n",
    "                                columns=['value'])\n",
    "\n",
    "    df.reset_index(inplace=True, names=\"catchment_id\")\n",
    "\n",
    "    # This should not be needed, but without memory usage grows\n",
    "    # del crosswalk_dict\n",
    "    # del f\n",
    "    # gc.collect()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e605d-3424-423c-985a-0e1312a9ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "        blob_name: str,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Retrieve a blob from the data service as xarray.Dataset.\n",
    "\n",
    "    Based largely on OWP HydroTools.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    blob_name: str, required\n",
    "        Name of blob to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : xarray.Dataset\n",
    "        The data stored in the blob.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"get_dataset: {blob_name}\")\n",
    "    \n",
    "    json_out = f\"{os.path.join(hu.get_cache_dir(), blob_name)}.json\"\n",
    "    hu.make_parent_dir(json_out )\n",
    "    blob_in = f\"gcs://national-water-model/{blob_name}\"\n",
    "    \n",
    "    gen_json2(blob_in, json_out)\n",
    "   \n",
    "    backend_args = { \"consolidated\": False,\n",
    "                     \"storage_options\": { \"fo\": json_out,\n",
    "                                    \"remote_protocol\": \"gcs\", \n",
    "                                    \"remote_options\": {'anon':True} }}\n",
    "    ds = xr.open_dataset(\n",
    "        \"reference://\", engine=\"zarr\",\n",
    "        backend_kwargs=backend_args\n",
    "    )\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a43380-1238-4533-9a8f-f6c51e18e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map_forcing(\n",
    "    blob_name: str,\n",
    "    weights_filepath: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculate the MAP for a single NetCDF file (i.e. one timestep).\n",
    "\n",
    "    ToDo: add way to filter which catchments are calculated\n",
    "    \"\"\"\n",
    "    # print(f\"Processing {blob_name}, {datetime.now()}\")\n",
    "\n",
    "    # Get some metainfo from blob_name\n",
    "    path_split = blob_name.split(\"/\")\n",
    "    reference_time = datetime.strptime(\n",
    "        path_split[0].split(\".\")[1] + path_split[2].split(\".\")[1],\n",
    "        \"%Y%m%dt%Hz\"\n",
    "    )\n",
    "    offset_hours = int(path_split[2].split(\".\")[4][1:])  # f001\n",
    "    value_time = reference_time + timedelta(hours=offset_hours)\n",
    "    configuration = path_split[1]\n",
    "\n",
    "    # Get xr.Dataset/xr.DataArray\n",
    "    ds = get_dataset(blob_name)\n",
    "    src = ds[\"RAINRATE\"]\n",
    "\n",
    "    # Pull out some attributes\n",
    "    measurement_unit = src.attrs[\"units\"]\n",
    "    variable_name = src.attrs[\"standard_name\"]\n",
    "\n",
    "    # Calculate MAP\n",
    "    df = calc_zonal_stats_weights(src, weights_filepath)\n",
    "\n",
    "    # Set metainfo for MAP\n",
    "    df[\"reference_time\"] = reference_time\n",
    "    df[\"value_time\"] = value_time\n",
    "    df[\"configuration\"] = configuration\n",
    "    df[\"measurement_unit\"] = measurement_unit\n",
    "    df[\"variable_name\"] = variable_name\n",
    "\n",
    "    # Reduce memory foot print\n",
    "    df['configuration'] = df['configuration'].astype(\"category\")\n",
    "    df['measurement_unit'] = df['measurement_unit'].astype(\"category\")\n",
    "    df['variable_name'] = df['variable_name'].astype(\"category\")\n",
    "    df[\"catchment_id\"] = df[\"catchment_id\"].astype(\"category\")\n",
    "\n",
    "    # print(df.info(verbose=True, memory_usage='deep'))\n",
    "    # print(df.memory_usage(index=True, deep=True))\n",
    "    # print(df)\n",
    "\n",
    "    # This should not be needed, but without memory usage grows\n",
    "    # ds.close()\n",
    "    # del ds\n",
    "    # gc.collect()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb971b24-2fcd-4de3-a110-aa65a7a44a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some criteria\n",
    "ingest_days = 30\n",
    "start_dt = datetime(2022, 12, 18) # First one is at 00Z in date\n",
    "td = timedelta(hours=6)\n",
    "number_of_forecasts = 1 #ingest_days * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dadbf7-7b45-4b05-8ab1-ad4141bb8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "\n",
    "# Loop though forecasts, fetch and insert\n",
    "for f in range(number_of_forecasts):\n",
    "    reference_time = start_dt + td * f\n",
    "    ref_time_str = reference_time.strftime(\"%Y%m%dT%HZ\")\n",
    "\n",
    "    print(f\"Processing: {ref_time_str}\")\n",
    "\n",
    "    blob_list = grid_to_parquet.list_blobs_forcing(\n",
    "        configuration = \"forcing_medium_range\",\n",
    "        reference_time = ref_time_str,\n",
    "        must_contain = \"forcing\"\n",
    "    )[:2]\n",
    "    \n",
    "    dfs = []\n",
    "    for blob_name in blob_list:\n",
    "        # df = calculate_map_forcing(\n",
    "        #     blob_name, \n",
    "        #     weights_filepath=config.HUC10_MEDIUM_RANGE_WEIGHTS_FILEPATH\n",
    "        # )\n",
    "        print(blob_name)\n",
    "        df = dask.delayed(calculate_map_forcing)(\n",
    "            blob_name, \n",
    "            weights_filepath=config.HUC10_MEDIUM_RANGE_WEIGHTS_FILEPATH\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Join all timesteps into single pd.DataFrame\n",
    "    results = dask.compute(*dfs)\n",
    "    df = pd.concat(results)\n",
    "    # df = pd.concat(dfs)\n",
    "\n",
    "    # Save as parquet file\n",
    "    parquet_filepath = os.path.join(config.MEDIUM_RANGE_FORCING_PARQUET, f\"{ref_time_str}.parquet\")\n",
    "    utils.make_parent_dir(parquet_filepath)\n",
    "    df.to_parquet(parquet_filepath)\n",
    "    \n",
    "    # del df\n",
    "    # gc.collect()\n",
    "\n",
    "    # Print out some DataFrame stats\n",
    "    # print(df.info(verbose=True, memory_usage='deep'))\n",
    "    # print(df.memory_usage(index=True, deep=True))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55111384-411c-45b9-8065-0f1a90b85ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
