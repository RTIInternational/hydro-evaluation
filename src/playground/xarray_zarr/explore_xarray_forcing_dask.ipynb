{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba89a5-212e-4cda-8b9f-86afe6abe2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# adding project dirs to path so code may be referenced from the notebook\n",
    "import sys\n",
    "sys.path.insert(0, '../../evaluation')\n",
    "sys.path.insert(0, '../../evaluation/queries')\n",
    "sys.path.insert(0, '../../evaluation/loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a88d62-8a46-4f48-a0ee-bd66bb3bad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fsspec\n",
    "import ujson\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import xarray as xr\n",
    "import dask\n",
    "import hvplot.xarray\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f801aa-28b1-4562-b398-b29f103a9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query some forcast data from parquet files\n",
    "import importlib\n",
    "import queries\n",
    "import config\n",
    "import utils\n",
    "importlib.reload(queries)\n",
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "import grid_to_parquet\n",
    "importlib.reload(grid_to_parquet)\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a89eb-5213-4a29-92ca-fe942c5f21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, progress\n",
    "\n",
    "cluster = LocalCluster(n_workers=16)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d29c19-5b5e-471e-8130-a3da5c44b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gcs', anon=True)\n",
    "fs2 = fsspec.filesystem('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06442ae-6182-4f1c-8af1-f58555ac32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = dict(mode='rb', anon=True, default_fill_cache=True, default_cache_type='first') # args to fs.open()\n",
    "# default_fill_cache=False avoids caching data in between file chunks to lowers memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467cbe-a192-49fc-88a3-b232921e5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_zarr_json(blob_name: str):\n",
    "    \"\"\"Given a blob_name path to GCS resource returns zarr JSON.\"\"\"\n",
    "    json_out = f\"{os.path.join(utils.get_cache_dir(), blob_name)}.json\"\n",
    "    utils.make_parent_dir(json_out )\n",
    "    blob_in = f\"gcs://national-water-model/{blob_name}\"\n",
    "    \n",
    "    with fs.open(blob_in, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, blob_in, inline_threshold=300)\n",
    "        with open(json_out, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "    return json_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44546d5a-98ef-42f1-abb0-edd840385dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zonal_stats_weights(\n",
    "    src: xr.DataArray,\n",
    "    weights_filepath: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculates zonal stats\"\"\"\n",
    "\n",
    "    crosswalk_dict = utils.read_weights_file(weights_filepath)\n",
    "        \n",
    "    r_array = src.values[0]\n",
    "    r_array[r_array == src.rio.nodata] = np.nan\n",
    "\n",
    "    mean_dict = {}\n",
    "    for key, value in crosswalk_dict.items():\n",
    "        mean_dict[key] = np.nanmean(r_array[value])\n",
    "\n",
    "    df = pd.DataFrame.from_dict(mean_dict,\n",
    "                                orient='index',\n",
    "                                columns=['value'])\n",
    "\n",
    "    df.reset_index(inplace=True, names=\"catchment_id\")\n",
    "\n",
    "    # This should not be needed, but without memory usage grows\n",
    "    # del crosswalk_dict\n",
    "    # del f\n",
    "    # gc.collect()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e605d-3424-423c-985a-0e1312a9ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "        zarr_json: str\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Retrieve a blob from the data service as xarray.Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    blob_name: str, required\n",
    "        Name of blob to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : xarray.Dataset\n",
    "        The data stored in the blob.\n",
    "\n",
    "    \"\"\"   \n",
    "    backend_args = { \"consolidated\": False,\n",
    "                     \"storage_options\": { \"fo\": zarr_json,\n",
    "                                    \"remote_protocol\": \"gcs\", \n",
    "                                    \"remote_options\": {'anon':True} }}\n",
    "    ds = xr.open_dataset(\n",
    "        \"reference://\", engine=\"zarr\",\n",
    "        backend_kwargs=backend_args\n",
    "    )\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a43380-1238-4533-9a8f-f6c51e18e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map_forcing(\n",
    "    zarr_json: str,\n",
    "    weights_filepath: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculate the MAP for a single NetCDF file (i.e. one timestep).\n",
    "\n",
    "    ToDo: add way to filter which catchments are calculated\n",
    "    \"\"\"\n",
    "\n",
    "#     # Get some metainfo from blob_name\n",
    "#     path_split = zarr_json.split(\"/\")\n",
    "#     reference_time = datetime.strptime(\n",
    "#         path_split[0].split(\".\")[1] + path_split[2].split(\".\")[1],\n",
    "#         \"%Y%m%dt%Hz\"\n",
    "#     )\n",
    "#     offset_hours = int(path_split[2].split(\".\")[4][1:])  # f001\n",
    "#     value_time = reference_time + timedelta(hours=offset_hours)\n",
    "#     configuration = path_split[1]\n",
    "    \n",
    "    # Get some metainfo from zarr_json\n",
    "    path_split = zarr_json.split(\"/\")\n",
    "    reference_time = datetime.strptime(\n",
    "        path_split[5].split(\".\")[1] + path_split[7].split(\".\")[1],\n",
    "        \"%Y%m%dt%Hz\"\n",
    "    )\n",
    "    offset_hours = int(path_split[7].split(\".\")[4][1:])  # f001\n",
    "    value_time = reference_time + timedelta(hours=offset_hours)\n",
    "    configuration = path_split[6]\n",
    "    \n",
    "    # Get xr.Dataset/xr.DataArray\n",
    "    ds = get_dataset(zarr_json)\n",
    "    src = ds[\"RAINRATE\"]\n",
    "\n",
    "    # Pull out some attributes\n",
    "    measurement_unit = src.attrs[\"units\"]\n",
    "    variable_name = src.attrs[\"standard_name\"]\n",
    "\n",
    "    # Calculate MAP\n",
    "    df = calc_zonal_stats_weights(\n",
    "        src,\n",
    "        weights_filepath\n",
    "    )\n",
    "\n",
    "    # Set metainfo for MAP\n",
    "    df[\"reference_time\"] = reference_time\n",
    "    df[\"value_time\"] = value_time\n",
    "    df[\"configuration\"] = configuration\n",
    "    df[\"measurement_unit\"] = measurement_unit\n",
    "    df[\"variable_name\"] = variable_name\n",
    "\n",
    "    # Reduce memory foot print\n",
    "    df['configuration'] = df['configuration'].astype(\"category\")\n",
    "    df['measurement_unit'] = df['measurement_unit'].astype(\"category\")\n",
    "    df['variable_name'] = df['variable_name'].astype(\"category\")\n",
    "    df[\"catchment_id\"] = df[\"catchment_id\"].astype(\"category\")\n",
    "\n",
    "    # print(df.info(verbose=True, memory_usage='deep'))\n",
    "    # print(df.memory_usage(index=True, deep=True))\n",
    "    # print(df)\n",
    "\n",
    "    # This should not be needed, but without memory usage grows\n",
    "    ds.close()\n",
    "    del ds\n",
    "    # gc.collect()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb971b24-2fcd-4de3-a110-aa65a7a44a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some criteria\n",
    "ingest_days = 30\n",
    "start_dt = datetime(2022, 12, 18, 18) # First one is at 00Z in date\n",
    "td = timedelta(hours=6)\n",
    "number_of_forecasts = 1 #ingest_days * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dadbf7-7b45-4b05-8ab1-ad4141bb8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop though forecasts, fetch and insert\n",
    "for f in range(number_of_forecasts):\n",
    "    reference_time = start_dt + td * f\n",
    "    ref_time_str = reference_time.strftime(\"%Y%m%dT%HZ\")\n",
    "\n",
    "    print(f\"Processing: {ref_time_str}\")\n",
    "\n",
    "    blob_list = grid_to_parquet.list_blobs_forcing(\n",
    "        configuration = \"forcing_medium_range\",\n",
    "        reference_time = ref_time_str,\n",
    "        must_contain = \"forcing\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Generate Zarr JSONS\n",
    "    time1 = time.time()\n",
    "    zarr_json_list = dask.compute(*[dask.delayed(gen_zarr_json)(b) for b in blob_list], retries=10)\n",
    "    time2 = time.time()\n",
    "    print(f\"Generate Zarr took: {time2-time1}\")\n",
    "    \n",
    "    # Calculate MAP\n",
    "    time1 = time.time()\n",
    "    dfs = []\n",
    "    for zarr_json in zarr_json_list:\n",
    "        df = dask.delayed(calculate_map_forcing)(\n",
    "            zarr_json, \n",
    "            weights_filepath=config.HUC10_MEDIUM_RANGE_WEIGHTS_FILEPATH\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Join all timesteps into single pd.DataFrame\n",
    "    results = dask.compute(*dfs)\n",
    "    df = pd.concat(results)\n",
    "\n",
    "    time2 = time.time()\n",
    "    print(f\"Download and Calculate MAP took: {time2-time1}\")\n",
    "    \n",
    "    # Save as parquet file\n",
    "    parquet_filepath = os.path.join(config.MEDIUM_RANGE_FORCING_PARQUET, f\"{ref_time_str}.parquet\")\n",
    "    utils.make_parent_dir(parquet_filepath)\n",
    "    df.to_parquet(parquet_filepath)\n",
    "    \n",
    "    # del df\n",
    "    # gc.collect()\n",
    "\n",
    "    # Print out some DataFrame stats\n",
    "    # print(df.info(verbose=True, memory_usage='deep'))\n",
    "    # print(df.memory_usage(index=True, deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1203c-8f61-4576-b3f1-557c1297b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54237ce0-5ba0-43c0-9367-d390c4daf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try combining to multizarr and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0731f-7a7d-4e68-9d7b-8ec7d0b2d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mzz = MultiZarrToZarr(\n",
    "    zarr_json_list,\n",
    "    remote_protocol='gcs',\n",
    "    remote_options={'anon':True},\n",
    "    concat_dims=['time'],\n",
    "    identical_dims = ['x', 'y'],\n",
    ")\n",
    "json_out = f\"{os.path.join(utils.get_cache_dir(), ref_time_str)}.json\"\n",
    "mzz.translate(json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab4e8-bf89-4c49-b0a5-ff3fddd30010",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset(json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dee058-651a-417c-a6c3-857634076887",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = ds.time.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e1ccb-6b43-417f-8475-8feb5ae10484",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfs2 = []\n",
    "for t in timesteps:\n",
    "    dfs2.append(dask.delayed(calc_zonal_stats_weights)(\n",
    "            src=ds.sel(time=[t])[\"RAINRATE\"], \n",
    "            weights_filepath=config.HUC10_MEDIUM_RANGE_WEIGHTS_FILEPATH\n",
    "        )\n",
    "    )\n",
    "results = dask.compute(*dfs2)\n",
    "df2 = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0710ca-6116-46ed-9d15-0d2a348f4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"catchment_id\"]==\"1016000606\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883b272-a6d2-454d-8b49-295c83c343e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(time=[t])[\"RAINRATE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037ad9c-47b6-4a8f-b273-437f9d8e05bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
