{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f28e53-63b8-48e1-8017-0c4c85997cae",
   "metadata": {},
   "source": [
    "## Post Event 1 - Explore Event Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436c6c8-f8cb-44be-ad20-7f7886813489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spatialpandas colormap colorcet duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfedb08-6f04-4659-934c-bf6946cfdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "sys.path.insert(0, '../../evaluation/')\n",
    "sys.path.insert(0, '../../evaluation/queries/')\n",
    "\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from evaluation import utils, config\n",
    "import queries\n",
    "from typing import List\n",
    "\n",
    "import hvplot.pandas  # noqa\n",
    "import holoviews as hv\n",
    "from holoviews import streams\n",
    "import geoviews as gv\n",
    "import spatialpandas as spd\n",
    "import datashader as ds\n",
    "import cartopy.crs as ccrs\n",
    "from holoviews.operation.datashader import (\n",
    "    rasterize, shade, regrid, inspect_points\n",
    ")\n",
    "from holoviews.operation.datashader import (\n",
    "    datashade, inspect_polygons\n",
    ")\n",
    "import colorcet as cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023e81a-d469-47fa-8962-f70f9bdf4820",
   "metadata": {},
   "source": [
    "### Static options (set once at start of session, independent of interactive selections) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131e77b-0534-4304-8f0c-fc97f483007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration (data sources)\n",
    "forcing_source = config.FORCING_ANALYSIS_ASSIM_PARQUET\n",
    "flow_source = config.USGS_PARQUET\n",
    "\n",
    "# Source of recurrence flow magnitudes per location\n",
    "recurrence_file = pathlib.Path(\"../data/nwm_v21_recurrence_flows_17C.nc\")  ## temporary, put in config if keeping\n",
    "high_flow_threshold = \"2_0_year_recurrence_flow_17C\"\n",
    "\n",
    "# gage upstream basin info - TEMPORARY, these boundaries are not good, have holes, etc.\n",
    "# eventually include other characteristic info - mean upstream slope, %imperv, soils, etc.\n",
    "gage_basin_info_file = pathlib.Path(\"../data/nwm_gage_basin_polygons.feather\")\n",
    "\n",
    "# source and header (resolution) of MAP polygons corresponding to data in 'forcing_source'\n",
    "polygon_file = pathlib.Path(\"../data/HUC10_Simp005_dd.geojson\")            ## temporary, eventually resolve which layer, how to simplify w/o gaps, \n",
    "polygon_id_header = \"HUC10\"                                                ## if/how to allow different MAP resolution...\n",
    "\n",
    "# source of HUC2 polygons - for reference only in maps\n",
    "huc2_file = pathlib.Path(\"../data/HUC2_Simp01_RemSPac.geojson\")\n",
    "\n",
    "# Units applies to all data\n",
    "# english -> inches for precp, cfs for flow\n",
    "# metric -> mm for precip, cms for flow\n",
    "units = \"english\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24747089-35d5-4e9a-aa98-2341e6add892",
   "metadata": {},
   "source": [
    "### Read static/independent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5137d9c-0e77-4296-8aed-c1e5428cde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in usgs points (if not already in memory - prevent annoying rereading)\n",
    "if not \"points_gdf\" in locals():\n",
    "    points_gdf = utils.get_usgs_gages()\n",
    "    points_gdf = points_gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "# read gage_basins, calculate area\n",
    "if not \"gage_basins_gdf\" in locals():\n",
    "    gage_basins = gpd.read_feather(gage_basin_info_file)\n",
    "    gage_basins['area_m2'] = gage_basins.to_crs(\"EPSG:3857\").geometry.area\n",
    "    \n",
    "# read in recurrence flows (if not already in memory - prevent annoying rereading)\n",
    "if not \"df_recurrence\" in locals():\n",
    "    ds_recurrence = xr.open_dataset(recurrence_file, engine=\"netcdf4\")\n",
    "    df_recurrence = ds_recurrence.to_dataframe()      \n",
    "    \n",
    "# read in polygons (if not already in memory - prevent annoying rereading)\n",
    "if not \"polygon_gdf\" in locals():\n",
    "    polygon_gdf = gpd.read_file(polygon_file).to_crs(\"EPSG:3857\")\n",
    "    polygon_gdf = polygon_gdf[[polygon_id_header,\"geometry\"]]\n",
    "    \n",
    "if not \"huc2_gdf\" in locals():\n",
    "    huc2_gdf_mult = gpd.read_file(huc2_file)\n",
    "    huc2_gdf = gpd.GeoDataFrame()\n",
    "    for i, polys in enumerate(huc2_gdf_mult.geometry):\n",
    "        row = huc2_gdf_mult.loc[[i],[\"huc2\",\"name\",\"geometry\"]]\n",
    "        for poly_part in polys.geoms:  \n",
    "            row[\"geometry\"] = poly_part\n",
    "            huc2_gdf = pd.concat([huc2_gdf, row], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf28f42-ca11-45ae-af60-6c28f3057f41",
   "metadata": {},
   "source": [
    "### Query-building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962be023-2c5c-4a51-b6eb-932ba0cc1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_filters(\n",
    "    source: str, \n",
    "    location_id_header: str,\n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    exclude_negative_values = True,\n",
    ") -> dict:\n",
    "    '''\n",
    "    Build filter portion of query to extract historical timeseries by region \n",
    "    (portion of ID) and value_time range\n",
    "    '''\n",
    "    filters = []\n",
    "    if location_id_string != \"all\":\n",
    "        filters.append(\n",
    "            {\n",
    "                \"column\": f\"{location_id_header}\",\n",
    "                \"operator\": \"like\",\n",
    "                \"value\": f\"{location_id_string}%\"\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        filters.append(\n",
    "            {\n",
    "                \"column\": f\"{location_id_header}\",\n",
    "                \"operator\": \"<>\",\n",
    "                \"value\": \"\"\n",
    "            }\n",
    "        )\n",
    "    if start_value_time is not None:\n",
    "        filters.append(\n",
    "            {\n",
    "                \"column\": \"value_time\",\n",
    "                \"operator\": \">=\",\n",
    "                \"value\": f\"{start_value_time}\"\n",
    "            }  \n",
    "        )\n",
    "    if end_value_time is not None:\n",
    "        filters.append(\n",
    "            {\n",
    "                \"column\": \"value_time\",\n",
    "                \"operator\": \"<=\",\n",
    "                \"value\": f\"{end_value_time}\"\n",
    "            }  \n",
    "        )\n",
    "    if exclude_negative_values:\n",
    "        filters.append(\n",
    "            {\n",
    "                \"column\": \"value\",\n",
    "                \"operator\": \">=\",\n",
    "                \"value\": 0\n",
    "            }  \n",
    "        )\n",
    "    return filters\n",
    "\n",
    "\n",
    "def get_historical_timeseries_query(\n",
    "    source: str, \n",
    "    location_id_header: str, \n",
    "    filters: List[dict]\n",
    ") -> str:    \n",
    "    '''\n",
    "    Build SQL query to extract historical timeseries by region \n",
    "    (portion of HUC ID) and value_time range\n",
    "    ''' \n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM read_parquet('{source}/*.parquet')\n",
    "        WHERE \n",
    "            {\" AND \".join(queries.format_filters(filters))}\n",
    "        ORDER BY\n",
    "            \"{location_id_header}\", value_time\n",
    "    ;\"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_historical_timeseries_chars_query(\n",
    "    source: str, \n",
    "    group_by: List[str],\n",
    "    order_by: List[str],\n",
    "    filters: List[dict]\n",
    ") -> str:    \n",
    "    '''\n",
    "    Build SQL query to extract characteristics of timeseries within\n",
    "    defined value_time range by region (portion of HUC ID) \n",
    "    '''    \n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            {\",\".join(group_by)},\n",
    "            sum(value) as sum,\n",
    "            max(value) as max,\n",
    "            min(value) as min,\n",
    "            mean(value) as mean,\n",
    "            var_pop(value) as variance,\n",
    "            any_value(measurement_unit) as units\n",
    "        FROM read_parquet('{source}/*.parquet')\n",
    "        WHERE \n",
    "            {\" AND \".join(queries.format_filters(filters))}\n",
    "        GROUP BY\n",
    "            {\",\".join(group_by)}\n",
    "        ORDER BY \n",
    "            {\",\".join(order_by)}\n",
    "    ;\"\"\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdee66-12e9-4982-b4dd-f7e2ba9c393e",
   "metadata": {},
   "source": [
    "### Query-running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb734860-48d3-4299-a5c7-3e1587806af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_timeseries(\n",
    "    source: str, \n",
    "    location_id_header: str,\n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Run DuckDB query to extract historical timeseries by region \n",
    "    (portion of ID) and value_time range\n",
    "    '''   \n",
    "    # build filters\n",
    "    filters = get_historical_filters(\n",
    "        source, \n",
    "        location_id_header, \n",
    "        location_id_string, \n",
    "        start_value_time, \n",
    "        end_value_time\n",
    "    )\n",
    "    #build query\n",
    "    query = get_historical_timeseries_query(\n",
    "        source, \n",
    "        location_id_header,\n",
    "        filters=filters\n",
    "    )\n",
    "    #run query\n",
    "    df = ddb.query(query).to_df()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_historical_timeseries_chars(\n",
    "    source: str, \n",
    "    location_id_header: str,\n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Run DuckDB query to extract characteristics of timeseries within\n",
    "    defined value_time range by region (portion of HUC ID) \n",
    "    '''\n",
    "    # build filters\n",
    "    filters = get_historical_filters(\n",
    "        source, \n",
    "        location_id_header, \n",
    "        location_id_string, \n",
    "        start_value_time, \n",
    "        end_value_time,\n",
    "    )\n",
    "    #build query    \n",
    "    query = get_historical_timeseries_chars_query(\n",
    "        source, \n",
    "        group_by=[location_id_header],\n",
    "        order_by=[location_id_header],\n",
    "        filters=filters\n",
    "    )\n",
    "    #run query    \n",
    "    df = ddb.query(query).to_df()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_parquet_date_range(source) -> pd.Timestamp:\n",
    "    '''\n",
    "    Query parquet files for defined fileset (source) and\n",
    "    return the min/max value_times in the files\n",
    "    '''    \n",
    "    query = f\"\"\"\n",
    "        SELECT count(distinct(value_time)) as count,\n",
    "        min(value_time) as start_time,\n",
    "        max(value_time) as end_time\n",
    "        FROM read_parquet('{source}/*.parquet')\n",
    "    ;\"\"\"\n",
    "    df = ddb.query(query).to_df()\n",
    "    return df.start_time[0], df.end_time[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991c688-57db-4055-9e05-f16ffa493ed5",
   "metadata": {},
   "source": [
    "### Data transform/processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c04451-51db-44fd-9beb-8da7f98df084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recurrence_interval(df_flow, df_recurr, flow_col_label = 'max'):\n",
    "    '''\n",
    "    Determine the highest defined recurrence interval flow that was exceeded by the max_flow\n",
    "    !!! currently assumes column headers are of the format\n",
    "        \"X_0_year_recurrence_flow\" and extracts the X value (as in nwm recurrence flow netcdf)\n",
    "    '''\n",
    "    # number of locations\n",
    "    n_locations = len(df_flow)\n",
    "\n",
    "    # get subset of recurrence flows for nwm_features in flow dataframe\n",
    "    df_recurr_sub = df_recurr.loc[df_flow['nwm_feature_id']]\n",
    "    recurr_flow_matrix = df_recurr_sub.to_numpy()\n",
    "    \n",
    "    # Get the recurrence intervals of the maximum flows\n",
    "\n",
    "    # create a tiled matrix of the recurrence intervals (years)\n",
    "    # repeating a row of the interval numbers, nlocations times\n",
    "    # **currently assumes column headers are of the format\n",
    "    #   \"X_0_year_recurrence_flow\" and extracts the X value\n",
    "    ncol = len(df_recurr_sub.columns)\n",
    "    recurr_labels = df_recurr_sub.columns.to_list()\n",
    "    recurr_vals = np.array([int(i.split(\"_\")[0]) for i in recurr_labels])\n",
    "    recurr_vals_tiled = np.tile(recurr_vals,(n_locations,1))    \n",
    "    \n",
    "    # create a tiled matrix of the maximum flow for each reach, \n",
    "    # repeating the max_flow column for each column of the recurr\n",
    "    # flow matrix for comparison     \n",
    "    flow_data = df_flow[flow_col_label]\n",
    "    flow_matrix = np.tile(flow_data,(ncol,1)).transpose() \n",
    "    \n",
    "    # get matrix of where the recurrence flows were exceeded by the max flow\n",
    "    exceed_recurr_flows = flow_matrix > recurr_flow_matrix\n",
    "\n",
    "    # Create a new matrix with values equal to the recurr interval value (years)\n",
    "    # if the max flow exceeded the recurrence flow (exceed_recurr_flows = True)\n",
    "    recurr_vals_matrix = np.where(exceed_recurr_flows, recurr_vals_tiled, 0)    \n",
    "    \n",
    "    # then find the maximum recurr interval exceeded by calculating the max\n",
    "    # across columns. This is the highest tabulated recurrence interval that \n",
    "    # was exceeded by the max flow (i.e., the recurrence category in the \n",
    "    # High Water Magnitude Product)\n",
    "    col_label = 'max_recurr_int'\n",
    "    df_flow[col_label] = np.amax(recurr_vals_matrix, axis = 1)    \n",
    "    \n",
    "    # reorder columns to put max_recurr_int next to max flow\n",
    "    cols = df_flow.columns.to_list()\n",
    "    i = cols.index(flow_col_label)\n",
    "    cols_reordered = cols[:i+1] + cols[-1:] + cols[i+1:-1] \n",
    "    #cols_reordered = cols[:i+1] + cols[-2:] + cols[i+1:-2]\n",
    "    df_flow = df_flow[cols_reordered].copy()    \n",
    "    \n",
    "    # add flag to indicate that all recurrence flows are equal (i.e., 2-yr flow = 100-yr flow), \n",
    "    # likely bad freq. analysis results\n",
    "    ind = df_recurr.loc[df_recurr.iloc[:,0] == df_recurr.iloc[:,-1]].index.to_list()\n",
    "    df_flow.loc[df_flow.index.isin(ind),'qual'] = 'recurr_all_equal'    \n",
    "    \n",
    "    return df_flow    \n",
    "\n",
    "\n",
    "def convert_units(\n",
    "    df: pd.DataFrame,\n",
    "    variable_name: str,\n",
    "    value_column: str,\n",
    "    units: str,    \n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # if forcing, convert from rate to depth, then check units\n",
    "    if variable_name == \"precipitation_flux\":       \n",
    "        df[value_column] = round(df[value_column]*60*60, 2)\n",
    "        if units == \"metric\" and df[\"units\"][0].find(\"mm\") < 0:\n",
    "            df[value_column] = df[value_column] * 25.4\n",
    "        elif units == \"english\" and df[\"units\"][0].find(\"in\") < 0:\n",
    "            df[value_column] = df[value_column] / 25.4\n",
    "            \n",
    "    if variable_name == \"streamflow\":\n",
    "        if units == \"metric\" and df[\"units\"][0].find(\"m\") < 0:\n",
    "            df[value_column] = df[value_column] * 0.0283\n",
    "        elif units == \"english\" and df[\"units\"][0].find(\"ft\") < 0:\n",
    "            df[value_column] = df[value_column] / 0.0283\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a63c9-6cbc-4c27-8773-0555f5e207ad",
   "metadata": {},
   "source": [
    "### Widgets definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53554d7-ebfa-400e-af42-4b4fd49330bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_date_range_slider(source):  \n",
    "    '''\n",
    "    Date range slider to select start and end dates of the event\n",
    "    '''\n",
    "    start_date, end_date = get_parquet_date_range(source)\n",
    "    event_dates_slider = pn.widgets.DatetimeRangeSlider(\n",
    "        name='Event start/end dates',\n",
    "        start=start_date, \n",
    "        end=end_date,\n",
    "        # default to start date plus 2 weeks\n",
    "        value=(start_date, start_date + timedelta(days = 14)),\n",
    "        step=1000*60*60,\n",
    "        bar_color = 'green',\n",
    "        width_policy=\"fit\"\n",
    "    )\n",
    "\n",
    "    return event_dates_slider\n",
    "\n",
    "def get_huc_selector():\n",
    "    '''\n",
    "    HUC2 region to explore, enables smaller region for faster responsiveness\n",
    "    '''\n",
    "    hucs=[\n",
    "        \"all\",\n",
    "        \"01\",\n",
    "        \"02\",\n",
    "        \"03\",\n",
    "        \"04\",\n",
    "        \"05\",\n",
    "        \"06\",\n",
    "        \"07\",\n",
    "        \"08\",\n",
    "        \"09\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "        \"12\",\n",
    "        \"13\",\n",
    "        \"14\",\n",
    "        \"15\",\n",
    "        \"16\",\n",
    "        \"17\",\n",
    "        \"18\",\n",
    "    ]\n",
    "    huc_selector = pn.widgets.Select(name='HUC2', options=hucs, value=\"all\", width_policy=\"fit\")\n",
    "    return huc_selector       \n",
    "\n",
    "def get_precip_measure_selector():\n",
    "    measures = [\n",
    "            \"sum\",\n",
    "            \"max\",\n",
    "            \"min\",\n",
    "        ]  \n",
    "    precip_measure_selector = pn.widgets.Select(name='Measure', options=measures, value=measures[0], width_policy=\"fit\") \n",
    "    return precip_measure_selector\n",
    "\n",
    "def get_flow_measure_selector():\n",
    "    measures = [\n",
    "            \"sum\",\n",
    "            \"max\",\n",
    "            \"min\",\n",
    "        ]  \n",
    "    flow_measure_selector = pn.widgets.Select(name='Measure', options=measures, value=measures[1], width_policy=\"fit\") \n",
    "    return flow_measure_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7b141-91ed-4786-9735-e70d09564644",
   "metadata": {},
   "source": [
    "### Utilities for plot style, background, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ed066-c72c-4e69-b137-aa3b7eaffdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precip_colormap():\n",
    "    ''' \n",
    "    build custom precip colormap \n",
    "    '''\n",
    "    cmap1 = cc.CET_L6[85:]\n",
    "    cmap2 = [cmap1[i] for i in range(0, len(cmap1), 3)]\n",
    "    ext = [cmap2[-1] + aa for aa in ['00','10','30','60','99']]\n",
    "    cmap = ext + cmap2[::-1] + cc.CET_R1\n",
    "    return cmap\n",
    "\n",
    "def get_recurr_colormap():\n",
    "    ''' \n",
    "    build explicit colormap for 2, 5, 10, 25, 50, 100 recurrence intervals\n",
    "    based on OWP High Flow Magnitude product\n",
    "    '''    \n",
    "    cmap = {0: 'lightgray', \n",
    "            2: 'dodgerblue', \n",
    "            5: 'yellow', \n",
    "            10: 'darkorange', \n",
    "            25: 'red', \n",
    "            50: 'fuchsia', \n",
    "            100: 'darkviolet'}\n",
    "    \n",
    "    return cmap\n",
    "    \n",
    "def get_basemap_gv(opts):\n",
    "    '''\n",
    "    get OSM basemap as geoviews object\n",
    "    '''\n",
    "    tiles = gv.tile_sources.OSM.opts(**opts)\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1142f-695b-4fc8-99f2-1e3c82f8d9f3",
   "metadata": {},
   "source": [
    "### Data selection and organizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bc720-562f-4a4f-b874-7062ef12cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregator(measure):\n",
    "    '''\n",
    "    datashader aggregator function\n",
    "    '''\n",
    "    return ds.mean(measure)\n",
    "\n",
    "def merge_df_with_gdf(\n",
    "    gdf, \n",
    "    geom_id_header: str, \n",
    "    df,\n",
    "    location_id_header: str, \n",
    ") -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    merge data df (result of DDB query) with geometry, return a geodataframe\n",
    "    '''\n",
    "    # merge df with geodataframe\n",
    "    merged_gdf = gdf.merge(df, left_on=geom_id_header, right_on=location_id_header)    \n",
    "\n",
    "    # if IDs are HUC codes, convert to type 'category'\n",
    "    if any(s in location_id_header for s in [\"HUC\",\"huc\"]):\n",
    "        print(f\"converting column {location_id_header} to category\")\n",
    "        merged_gdf[location_id_header] = merged_gdf[location_id_header].astype(\"category\")\n",
    "    \n",
    "    return merged_gdf\n",
    "\n",
    "def subset_df_by_measure_min_max(\n",
    "    df: pd.DataFrame,\n",
    "    measure: str,\n",
    "    measure_min: float = None,\n",
    "    measure_max: float = None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # subset data based on min/max (if any)\n",
    "    subset_df = df.copy()\n",
    "    if measure_min:\n",
    "        subset_df = df[df[measure] >= measure_min]\n",
    "    if measure_max:\n",
    "        subset_df = subset_df[subset_df[measure] <= measure_max]\n",
    "        \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c3442-fd5a-49d6-911e-b73333c11693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_timeseries_chars_merged_gdf(\n",
    "    gdf, \n",
    "    geom_id_header: str, \n",
    "    source: str, \n",
    "    location_id_header: str, \n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = 'streamflow',    \n",
    "    measure: str = 'max',\n",
    "    units = 'english',\n",
    "    measure_min = None,\n",
    "    measure_max = None,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    query data\n",
    "    convert/transform/process data\n",
    "    merge with geometry\n",
    "    '''\n",
    "    # query timeseries characteristics\n",
    "    df = get_historical_timeseries_chars(\n",
    "        source, \n",
    "        location_id_header, \n",
    "        location_id_string, \n",
    "        start_value_time, \n",
    "        end_value_time,\n",
    "    )\n",
    "    # convert units if needed  \n",
    "    converted_df = convert_units(\n",
    "        df, \n",
    "        variable_name,\n",
    "        measure,\n",
    "        units,\n",
    "    )      \n",
    "    # subset data based on min/max (if any)\n",
    "    subset_df = subset_df_by_measure_min_max(\n",
    "        converted_df,\n",
    "        measure,\n",
    "        measure_min,\n",
    "        measure_max,\n",
    "    )\n",
    "        \n",
    "    # merge with geodataframe    \n",
    "    merged_gdf = merge_df_with_gdf(\n",
    "        gdf, \n",
    "        geom_id_header, \n",
    "        subset_df, \n",
    "        location_id_header\n",
    "    )        \n",
    "    # add recurrence flow levels of the peak flows\n",
    "    if variable_name == \"streamflow\":\n",
    "        merged_gdf = add_recurrence_interval(merged_gdf, df_recurrence, flow_col_label = 'max')\n",
    "        merged_gdf = merged_gdf.merge(df_recurrence[[\"2_0_year_recurrence_flow_17C\"]], right_on = \"feature_id\", left_on = \"nwm_feature_id\")\n",
    "        merged_gdf['max_rel_to_2yr'] = merged_gdf[\"max\"]/merged_gdf[\"2_0_year_recurrence_flow_17C\"]\n",
    "        \n",
    "    return merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4bfc6-fe6b-4282-8462-f2a897200087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_date, end_date = get_parquet_date_range(forcing_source)\n",
    "#event_dates_slider = get_event_date_range_slider(forcing_source)\n",
    "huc_selector = get_huc_selector()\n",
    "#precip_measure_selector = get_precip_measure_selector()\n",
    "flow_measure_selector = get_flow_measure_selector()\n",
    "\n",
    "merged_gdf = get_historical_timeseries_chars_merged_gdf(\n",
    "    gdf = points_gdf,\n",
    "    geom_id_header = \"gage_id\",\n",
    "    source = flow_source,    \n",
    "    location_id_header = \"usgs_site_code\",\n",
    "    location_id_string = \"\", \n",
    "    start_value_time = start_date, #event_dates_slider.param.value_start,\n",
    "    end_value_time = end_date, #event_dates_slider.param.value_end,\n",
    "    variable_name = \"streamflow\",\n",
    "    measure = \"max\", #flow_measure_selector.param.value,\n",
    "    units = units,\n",
    "    measure_min = 0.01,\n",
    "    measure_max = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959016e8-bbdb-4ff0-8119-f7823005abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works\n",
    "\n",
    "measure = 'max'\n",
    "points_df = merged_gdf.loc[:,['longitude','latitude','gage_id',measure]]\n",
    "points_hv = hv.Points(points_df, vdims=[measure])\n",
    "points_hv.opts(width=700, color=hv.dim(measure), cmap='viridis_r', colorbar=True, cnorm='log')\n",
    "hist = points_df.hvplot(width=700, y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(points_hv + hist)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15a46e-1481-460f-a3d1-f8b734f1d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works- switch to recurrence int\n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "points_df = merged_gdf.loc[:,['longitude','latitude','gage_id',measure]]\n",
    "points_hv2 = hv.Points(points_df, vdims=[measure])\n",
    "points_hv2.opts(width=700, color=hv.dim(measure), cmap='viridis_r', colorbar=True, cnorm='log')\n",
    "hist2 = points_df.hvplot(width=700, y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(points_hv2 + hist2)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31259ba0-6829-4d3e-864d-56642bd5db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works - custom colors\n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "points_df = merged_gdf.loc[:,['longitude','latitude','gage_id',measure]]\n",
    "points_df = points_df.sort_values(measure, ascending = True)\n",
    "\n",
    "points_hv3 = hv.Points(points_df, vdims=[measure])\n",
    "points_hv3.opts(width=700, color=hv.dim(measure), cmap=get_recurr_colormap(), colorbar=True, size = 5)\n",
    "peaks_hist3 = points_df.hvplot(width=700, y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(points_hv3 + peaks_hist3)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2203b29-8189-4955-b4f5-fb471d897713",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works - separate out zero point layer\n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "points_df = merged_gdf.loc[:,['longitude','latitude','gage_id',measure]]\n",
    "points_df = points_df.sort_values(measure, ascending = False)\n",
    "points_sub_df = points_df[points_df[measure] > 0]\n",
    "points_0_df = points_df[points_df[measure] == 0].copy()\n",
    "\n",
    "points_sub_hv = hv.Points(points_sub_df, vdims=[measure])\n",
    "points_sub_hv.opts(width=700, height=400, color=hv.dim(measure), cmap=get_recurr_colormap(), legend_position = 'bottom_right', size = 5)\n",
    "\n",
    "points_0_hv = hv.Points(points_0_df, vdims=[measure])\n",
    "points_0_hv.opts(width=700, color=hv.dim(measure), cmap=['lightgray'], show_legend = False, size = 2)\n",
    "\n",
    "peaks_sub_hist = points_sub_df.hvplot(width=700, y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(points_0_hv * points_sub_hv + peaks_sub_hist)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c551f7-ef90-4cb4-a35c-37f243161ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b34687-b25f-4da4-990e-684e88ec0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works but EXTREMELY slow (geodataframe instead of dataframe)\n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "points_df = merged_gdf    #.loc[:,['longitude','latitude','gage_id',measure]]    \n",
    "points_df = points_df.sort_values(measure, ascending = False)\n",
    "points_sub_df = points_df[points_df[measure] > 0]\n",
    "points_0_df = points_df[points_df[measure] == 0].copy()\n",
    "\n",
    "points_sub_hv = hv.Points(points_sub_df, vdims=[measure])\n",
    "points_sub_hv.opts(width=700, height=400, color=hv.dim(measure), cmap=get_recurr_colormap(), legend_position = 'bottom_right', size = 5)\n",
    "\n",
    "points_0_hv = hv.Points(points_0_df, vdims=[measure])\n",
    "points_0_hv.opts(width=700, color=hv.dim(measure), cmap=['lightgray'], show_legend = False, size = 2)\n",
    "\n",
    "peaks_sub_hist = points_sub_df.hvplot(width=700, y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(points_0_hv * points_sub_hv + peaks_sub_hist)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8709409-ccb2-47d3-a1e7-865552648e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9e7f1-b4a5-45c9-9267-96d32557d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points_sub_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402eaff-c236-4fb5-a0e4-2516660205ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works faster - convert to spatialpandas geodataframe\n",
    "## layout shifts to the right for some reason\n",
    "\n",
    "points_df = spd.GeoDataFrame(merged_gdf)   \n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "points_sdf = spd.GeoDataFrame(merged_gdf)   \n",
    "points_sdf = points_sdf.sort_values(measure, ascending = False)\n",
    "points_sub_sdf = points_sdf[points_sdf[measure] > 0]\n",
    "points_0_sdf = points_sdf[points_sdf[measure] == 0].copy()\n",
    "points_sub_hv = hv.Points(points_sub_sdf, vdims=[measure])\n",
    "\n",
    "points_sub_hv = hv.Points(points_sub_sdf, vdims=[measure])\n",
    "points_sub_hv.opts(width=700, height=400, color=hv.dim(measure), cmap=get_recurr_colormap(), legend_position = 'bottom_right', size = 5)\n",
    "\n",
    "points_0_hv = hv.Points(points_0_sdf, vdims=[measure])\n",
    "points_0_hv.opts(width=700, color=hv.dim(measure), cmap=['lightgray'], show_legend = False, size = 2)\n",
    "\n",
    "# build histogram directly from points holoview obj\n",
    "points_sub_hist = hv.operation.histogram(points_sub_hv, bin_range=(2, 100), dimension=measure) \n",
    "points_sub_hist.opts(width=700)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(points_0_hv * points_sub_hv + points_sub_hist)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdfa11-4f34-4401-8f44-48f27bb4dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works faster - convert to spatialpandas geodataframe, use panel for layout\n",
    "\n",
    "from holoviews.element import tiles\n",
    "esri = tiles.ESRI().redim(x='easting', y='northing')\n",
    "osm = tiles.OSM().redim(x='easting', y='northing')\n",
    "\n",
    "points_df = spd.GeoDataFrame(merged_gdf)   \n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "points_sdf = spd.GeoDataFrame(merged_gdf)   \n",
    "points_sdf = points_sdf.sort_values(measure, ascending = False)\n",
    "points_sub_sdf = points_sdf[points_sdf[measure] > 0]\n",
    "points_0_sdf = points_sdf[points_sdf[measure] == 0].copy()\n",
    "points_sub_hv = hv.Points(points_sub_sdf, vdims=[measure])\n",
    "\n",
    "points_sub_hv = hv.Points(points_sub_sdf, vdims=[measure])\n",
    "points_sub_hv.opts(width=700, height=400, color=hv.dim(measure), cmap=get_recurr_colormap(), legend_position = 'bottom_right', size = 5)\n",
    "\n",
    "points_0_hv = hv.Points(points_0_sdf, vdims=[measure])\n",
    "points_0_hv.opts(width=700, color=hv.dim(measure), cmap=['lightgray'], show_legend = False, size = 2)\n",
    "\n",
    "points_sub_hist = hv.operation.histogram(points_sub_hv, bin_range=(2, 100), dimension=measure) \n",
    "points_sub_hist.opts(width=700)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "map_layout = (ls(osm * points_0_hv * points_sub_hv + points_sub_hist)).cols(1)\n",
    "\n",
    "pn_layout = pn.Column(map_layout)\n",
    "pn_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cacfa-15a9-4356-b3ea-c964e8fb120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(points_sub_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f8cb2-3fc5-4139-983c-e7973dd4858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points_sub_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602bf27-2331-472e-ac98-8f8add0684a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This works - must use easting/northing to overlay dataframe with basemap\n",
    "\n",
    "from holoviews.element import tiles\n",
    "esri = tiles.ESRI().redim(x='easting', y='northing')\n",
    "osm = tiles.OSM().redim(x='easting', y='northing')\n",
    "\n",
    "measure = 'max_recurr_int'\n",
    "merged_gdf['easting'] = merged_gdf.geometry.x\n",
    "merged_gdf['northing'] = merged_gdf.geometry.y\n",
    "points_df = merged_gdf.loc[:,['easting','northing','gage_id',measure]]\n",
    "points_df = points_df.sort_values(measure, ascending = False)\n",
    "points_sub_df = points_df[points_df[measure] > 0]\n",
    "points_0_df = points_df[points_df[measure] == 0].copy()\n",
    "\n",
    "frame_width = 700\n",
    "points_sub_hv = hv.Points(points_sub_df, vdims=[measure])\n",
    "points_sub_hv.opts(\n",
    "    frame_width=frame_width, frame_height = round(frame_width * 4/7), color=hv.dim(measure), \n",
    "    cmap=get_recurr_colormap(), legend_position = 'bottom_right', size = 5, xaxis=None, yaxis=None, toolbar='right')\n",
    "\n",
    "points_0_hv = hv.Points(points_0_df, vdims=[measure])\n",
    "points_0_hv.opts(color=hv.dim(measure), cmap=['lightgray'], show_legend = False, size = 2)\n",
    "\n",
    "peaks_sub_hist = points_sub_df.hvplot(frame_width=frame_width, y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "ls = hv.link_selections.instance()\n",
    "layout = (ls(osm * points_0_hv * points_sub_hv + peaks_sub_hist)).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a48286-a589-4054-8002-4dffd7ab1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(points_sub_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6f3fd-e556-450f-ac5f-33bd416656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points_sub_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca8796-fb7e-4440-89f5-e4f032eebb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and select subset from dataset\n",
    "#  cannot alter size with width and height... \n",
    "\n",
    "from holoviews.element import tiles\n",
    "esri = tiles.ESRI().redim(x='easting', y='northing')\n",
    "osm2 = tiles.OSM().redim(x='easting', y='northing')\n",
    "\n",
    "width = 700\n",
    "\n",
    "measures = ['sum','max','min','mean','variance','max_recurr_int']\n",
    "vdims = measures + ['gage_id','easting','northing']\n",
    "cols = vdims + ['latitude','longitude']\n",
    "kdims = ['longitude','latitude']\n",
    "\n",
    "df = merged_gdf.loc[:,cols]\n",
    "data = hv.Dataset(df, kdims, vdims)\n",
    "data_abv = data.select(max_recurr_int=(1,101))\n",
    "data_0 = data.select(max_recurr_int=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9100b0-a952-4da3-b5f1-69eff185f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = data.to(hv.Points, ['longitude', 'latitude'], 'max_recurr_int')\n",
    "points.opts(color=hv.dim(measure), cmap=get_recurr_colormap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f907274-d5b3-4aa7-b869-5a92c83cf0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = data_abv.to(hv.Points, ['longitude', 'latitude'], 'max_recurr_int')\n",
    "points.opts(color=hv.dim(measure), cmap=get_recurr_colormap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af82306-91f7-4c8e-b484-38ee69ddb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = get_parquet_date_range(forcing_source)\n",
    "#event_dates_slider = get_event_date_range_slider(forcing_source)\n",
    "huc_selector = get_huc_selector()\n",
    "#precip_measure_selector = get_precip_measure_selector()\n",
    "flow_measure_selector = get_flow_measure_selector()\n",
    "\n",
    "merged_gdf = get_historical_timeseries_chars_merged_gdf(\n",
    "    gdf = points_gdf,\n",
    "    geom_id_header = \"gage_id\",\n",
    "    source = flow_source,    \n",
    "    location_id_header = \"usgs_site_code\",\n",
    "    location_id_string = \"\", \n",
    "    start_value_time = start_date, #event_dates_slider.param.value_start,\n",
    "    end_value_time = end_date, #event_dates_slider.param.value_end,\n",
    "    variable_name = \"streamflow\",\n",
    "    measure = \"max\", #flow_measure_selector.param.value,\n",
    "    units = units,\n",
    "    measure_min = 0.01,\n",
    "    measure_max = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d91100-9aa7-4745-b965-572a53ae5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_sdf['easting'] = merged_sdf.geometry.x\n",
    "merged_sdf['northing'] = merged_sdf.geometry.y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30daa8bd-5ee0-4a95-9d8f-bb8fca44a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = 'max_recurr_int'\n",
    "merged_gdf['easting'] = merged_gdf.geometry.x\n",
    "merged_gdf['northing'] = merged_gdf.geometry.y\n",
    "keepcols = points_df.columns[~points_df.columns.isin(['geometry'])]\n",
    "vdimcols = points_df.columns[~points_df.columns.isin(['easting','northing', measure])]\n",
    "\n",
    "points_df = merged_gdf.loc[:,keepcols]\n",
    "points_df = points_df.sort_values(measure, ascending = False)\n",
    "points_sub_df = points_df[points_df[measure] > 0]\n",
    "points_0_df = points_df[points_df[measure] == 0].copy()\n",
    "\n",
    "points_sub_hv2 = hv.Points(points_sub_df, vdims=([measure] + vdimcols.to_list()))\n",
    "points_sub_hv2.opts(\n",
    "    width=500, \n",
    "    height = round(width * 4/7), \n",
    "    color=hv.dim(measure),     \n",
    "    #aspect = 7/4, responsive = True, \n",
    "    cmap=get_recurr_colormap(), \n",
    "#        legend_position = 'right', legend_offset=(0,0), \n",
    "        legend_position = 'bottom_right',\n",
    "        size = 5, xaxis=None, yaxis=None, toolbar='right')\n",
    "\n",
    "points_0_hv2 = hv.Points(points_0_df, vdims=([measure] + vdimcols.to_list()))\n",
    "points_0_hv2.opts(color=hv.dim(measure), cmap=['lightgray'], show_legend = False, size = 2)\n",
    "\n",
    "emin, emax = points_sub_df['easting'].min(), points_sub_df['easting'].max()\n",
    "nmin, nmax = points_sub_df['northing'].min(), points_sub_df['northing'].max()\n",
    "\n",
    "peaks_sub_hist2 = points_sub_df.hvplot(y=measure, kind='hist', responsive=True, min_height=200)\n",
    "\n",
    "df = merged_gdf[['gage_id','sum','max','min','mean','variance','max_recurr_int']]\n",
    "scatter2 = hv.Scatter(df, 'max', 'min')\n",
    "scatter2.opts(responsive = True, toolbar = 'above', size = 5)\n",
    "\n",
    "pn.extension(sizing_mode='stretch_width')\n",
    "ls = hv.link_selections.instance()\n",
    "layout_linked_hist = ((ls(osm2 * points_0_hv2[emin:emax, nmin:nmax] * points_sub_hv2[emin:emax, nmin:nmax] + peaks_sub_hist2)).cols(1))\n",
    "layout = \\\n",
    "    pn.Column(\n",
    "        pn.Row(\n",
    "            pn.pane.PNG('https://ciroh.ua.edu/wp-content/uploads/2022/08/CIROHLogo_200x200.png', width=100),\n",
    "            pn.pane.Markdown(\n",
    "                \"\"\"\n",
    "                # CIROH Exploratory Evaluation Toolset\n",
    "                ## Post-Event Observed Data Exploration\n",
    "                \"\"\",\n",
    "                width=800\n",
    "            ),\n",
    "        ),\n",
    "        pn.Row(\n",
    "            scatter2,\n",
    "            layout_linked_hist,\n",
    "            ),\n",
    "    )\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a9d73-6c6e-4069-95a3-15ac509c26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build hist and scat off of points\n",
    "\n",
    "measures = ['sum','max','min','mean','variance','max_recurr_int']\n",
    "vdims = measures + ['gage_id','easting','northing']\n",
    "cols = vdims + ['latitude','longitude']\n",
    "kdims = ['longitude','latitude']\n",
    "\n",
    "df_ds = merged_gdf.loc[:,cols]\n",
    "data = hv.Dataset(df_ds, kdims, vdims)\n",
    "\n",
    "points = hv.Points(data, kdims = ['longitude','latitude'], vdims = ['max_recurr_int', 'sum','max','min','mean','variance'])\n",
    "points.opts(width = 700, height=400, color=hv.dim(measure), cmap=get_recurr_colormap())\n",
    "\n",
    "hist = hv.operation.histogram(points, bin_range=(2, 100), dimension = 'max_recurr_int')\n",
    "\n",
    "scat = hv.Scatter(df_ds, kdims = ['sum'], vdims = ['max','max_recurr_int','latitude','longitude'])\n",
    "\n",
    "pn.extension(sizing_mode='stretch_width')\n",
    "ls = hv.link_selections.instance()\n",
    "linked = ls(scat + points + hist)\n",
    "linked.opts(toolbar='right')\n",
    "\n",
    "layout = pn.Row(linked)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f74254-b4de-4c60-9457-5f2e7784ab18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
