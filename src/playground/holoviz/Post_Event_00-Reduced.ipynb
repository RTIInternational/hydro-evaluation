{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f28e53-63b8-48e1-8017-0c4c85997cae",
   "metadata": {},
   "source": [
    "## Post Event 1 - Explore Event Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436c6c8-f8cb-44be-ad20-7f7886813489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spatialpandas colormap colorcet duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfedb08-6f04-4659-934c-bf6946cfdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "sys.path.insert(0, '../../evaluation/')\n",
    "sys.path.insert(0, '../../evaluation/queries/')\n",
    "\n",
    "from evaluation import utils, config\n",
    "import temp_queries\n",
    "import temp_data_utils\n",
    "import temp_eval_dashboard_utils as temp_dash_utils\n",
    "import importlib\n",
    "\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "import colorcet as cc\n",
    "#import hvplot.pandas  \n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import spatialpandas as spd\n",
    "import datashader as ds\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Point\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.operation.datashader import inspect_polygons\n",
    "\n",
    "hv.extension('bokeh', logo=False)\n",
    "pn.extension(sizing_mode='stretch_width')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023e81a-d469-47fa-8962-f70f9bdf4820",
   "metadata": {},
   "source": [
    "### Static options (set once at start of session, independent of interactive selections) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131e77b-0534-4304-8f0c-fc97f483007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration (data sources)\n",
    "forcing_source = config.FORCING_ANALYSIS_ASSIM_PARQUET\n",
    "flow_source = config.USGS_PARQUET\n",
    "\n",
    "# Source of recurrence flow magnitudes per location\n",
    "recurrence_file = pathlib.Path(\"../data/nwm_v21_recurrence_flows_17C.nc\")  ## temporary, put in config if keeping\n",
    "high_flow_threshold = \"2_0_year_recurrence_flow_17C\"\n",
    "\n",
    "# gage upstream basin info - TEMPORARY, these boundaries are not good, have holes, etc.\n",
    "# eventually include other characteristic info - mean upstream slope, %imperv, soils, etc.\n",
    "gage_basin_info_file = pathlib.Path(\"../data/nwm_gage_basin_polygons.feather\")\n",
    "\n",
    "# source and header (resolution) of MAP polygons corresponding to data in 'forcing_source'\n",
    "polygon_file = pathlib.Path(\"../data/HUC10_Simp005_dd.geojson\")            ## temporary, eventually resolve which layer, how to simplify w/o gaps, \n",
    "polygon_id_header = \"HUC10\"                                                ## if/how to allow different MAP resolution...\n",
    "\n",
    "# source of HUC2 polygons - for reference only in maps\n",
    "huc2_file = pathlib.Path(\"../data/HUC2_Simp01_RemSPac.geojson\")\n",
    "\n",
    "# Units applies to all data\n",
    "# english -> inches for precp, cfs for flow\n",
    "# metric -> mm for precip, cms for flow\n",
    "current_units = \"english\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24747089-35d5-4e9a-aa98-2341e6add892",
   "metadata": {},
   "source": [
    "### Read static/independent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5137d9c-0e77-4296-8aed-c1e5428cde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in usgs points (if not already in memory - prevent annoying rereading)\n",
    "if not \"points_gdf\" in locals():\n",
    "    points_gdf = utils.get_usgs_gages()\n",
    "    points_gdf = points_gdf.to_crs(\"EPSG:3857\")\n",
    "    # add easting and northing - helpful for plotting as points on basemap\n",
    "    points_gdf['easting'] = points_gdf.geometry.x\n",
    "    points_gdf['northing'] = points_gdf.geometry.y       \n",
    "    \n",
    "# read gage_basins, calculate area\n",
    "if not \"gage_basins_gdf\" in locals():\n",
    "    gage_basins = gpd.read_feather(gage_basin_info_file)\n",
    "    gage_basins['area_m2'] = gage_basins.to_crs(\"EPSG:3857\").geometry.area\n",
    "    \n",
    "# read in recurrence flows (if not already in memory - prevent annoying rereading)\n",
    "# recurrence flows are in units of CFS\n",
    "if not \"recurrence_flows_df\" in locals():\n",
    "    recurrence_flows_ds = xr.open_dataset(recurrence_file, engine=\"netcdf4\")\n",
    "    recurrence_flows_df = recurrence_flows_ds.to_dataframe()      \n",
    "    \n",
    "# read in polygons (if not already in memory - prevent annoying rereading)\n",
    "if not \"polygons_gdf\" in locals():\n",
    "    polygons_gdf = gpd.read_file(polygon_file).to_crs(\"EPSG:3857\")\n",
    "    polygons_gdf = polygons_gdf[[polygon_id_header,\"geometry\"]]\n",
    "    \n",
    "if not \"huc2_gdf\" in locals():\n",
    "    huc2_gdf_mult = gpd.read_file(huc2_file)\n",
    "    huc2_gdf = gpd.GeoDataFrame()\n",
    "    for i, polys in enumerate(huc2_gdf_mult.geometry):\n",
    "        row = huc2_gdf_mult.loc[[i],[\"huc2\",\"name\",\"geometry\"]]\n",
    "        for poly_part in polys.geoms:  \n",
    "            row[\"geometry\"] = poly_part\n",
    "            huc2_gdf = pd.concat([huc2_gdf, row], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13280c64-c8b1-4e91-8cb3-8de6616a7d1a",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54baba-43fd-45fd-969e-cec7f9f64274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_timeseries_chars_with_geom(\n",
    "    gdf, \n",
    "    geom_id_header: str, \n",
    "    source: str, \n",
    "    location_id_header: str, \n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = 'streamflow',    \n",
    "    units = 'english',    \n",
    "    measure: str = 'max',\n",
    "    measure_min_allowable = None,\n",
    "    measure_max_allowable = None,\n",
    "    recurrence_flows_df: pd.DataFrame() = None,\n",
    "    high_flow_threshold: str = \"2_0_year_recurrence_flow_17C\",\n",
    ") -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    query data\n",
    "    convert/transform/process data\n",
    "    merge with geometry\n",
    "    '''\n",
    "    # query timeseries characteristics\n",
    "    df = temp_dash_utils.get_historical_timeseries_chars(\n",
    "        source, \n",
    "        location_id_header, \n",
    "        location_id_string, \n",
    "        start_value_time, \n",
    "        end_value_time,\n",
    "    )\n",
    "    # convert units if needed  \n",
    "    all_measures = df.columns[~df.columns.isin([location_id_header, 'units'])].to_list()\n",
    "    converted_df = temp_data_utils.convert_units(\n",
    "        df, \n",
    "        variable_name,\n",
    "        all_measures,\n",
    "        units,\n",
    "    )      \n",
    "    # merge with geodataframe (must do this before adding recurrence flows so have the nwm_feature_id)\n",
    "    merged_gdf = temp_data_utils.merge_df_with_gdf(\n",
    "        gdf, \n",
    "        geom_id_header, \n",
    "        converted_df, \n",
    "        location_id_header\n",
    "    )                \n",
    "    # if streamflow, add recurrence flow levels of the peak flows\n",
    "    if variable_name == \"streamflow\":\n",
    "        if not recurrence_flows_df.empty:\n",
    "            merged_gdf = temp_data_utils.add_recurrence_interval(merged_gdf, recurrence_flows_df, flow_col_label = 'max')\n",
    "            #converted_df = converted_df.merge(recurrence_flows_df[[high_flow_threshold]], right_on = \"feature_id\", left_on = \"nwm_feature_id\")\n",
    "            all_measures = all_measures + ['max_recurr_int']        \n",
    "    \n",
    "    # subset data based on min/max (if any)\n",
    "    subset_gdf = temp_data_utils.subset_df_by_measure_min_max(\n",
    "        merged_gdf,\n",
    "        measure,\n",
    "        measure_min_allowable,\n",
    "        measure_max_allowable,\n",
    "    )\n",
    "    # reduce columns\n",
    "    keep_cols = [location_id_header,'geometry','units'] + all_measures\n",
    "    if subset_gdf.geom_type.values[0] == 'Point':\n",
    "        keep_cols = keep_cols + ['latitude','longitude','easting','northing']\n",
    "    subset_gdf = subset_gdf.loc[:,keep_cols]    \n",
    "        \n",
    "    return subset_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb0399-cdd1-4674-b7a5-0c45d486ad1e",
   "metadata": {},
   "source": [
    "### Holoviews object definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895d710-90e8-4eeb-9b0b-b6aba27542aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_timeseries_chars_polygons_hv(\n",
    "    gdf, \n",
    "    geom_id_header: str, \n",
    "    source: str, \n",
    "    location_id_header: str, \n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = 'streamflow',  \n",
    "    session_units = 'english',    \n",
    "    measure: str = 'max',\n",
    "    measure_min_allowable = None,\n",
    "    measure_max_allowable = None,\n",
    ") -> hv.Element:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # get data with geometry\n",
    "    merged_gdf = get_historical_timeseries_chars_with_geom(\n",
    "        gdf = gdf,\n",
    "        geom_id_header = geom_id_header,\n",
    "        source = source,    \n",
    "        location_id_header = location_id_header,\n",
    "        location_id_string = location_id_string, \n",
    "        start_value_time = start_value_time,\n",
    "        end_value_time = end_value_time,\n",
    "        variable_name = variable_name,\n",
    "        units = session_units,        \n",
    "        measure = measure,\n",
    "        measure_min_allowable = measure_min_allowable,\n",
    "        measure_max_allowable = measure_max_allowable,\n",
    "    )\n",
    "    # get min/max values of the extracted data\n",
    "    measure_min_in_dataset = merged_gdf[measure].min()\n",
    "    measure_max_in_dataset = merged_gdf[measure].max()    \n",
    "          \n",
    "    #convert to spatialpandas object (required for inspect polygons function)\n",
    "    merged_sdf = spd.GeoDataFrame(merged_gdf)         \n",
    "\n",
    "    # declare polygon geoviews object   \n",
    "    polygons_hv = gv.Polygons(\n",
    "        merged_sdf,\n",
    "        crs=ccrs.GOOGLE_MERCATOR, \n",
    "        vdims=[measure, location_id_header]\n",
    "    )           \n",
    "    # reset the data range based on data in the current sample\n",
    "    polygons_hv = polygons_hv.redim.range(**{f\"{measure}\": (measure_min_in_dataset, measure_max_in_dataset)})\n",
    "        \n",
    "    return polygons_hv    \n",
    "\n",
    "def get_historical_timeseries_chars_points_hv(\n",
    "    gdf, \n",
    "    geom_id_header: str, \n",
    "    source: str, \n",
    "    location_id_header: str, \n",
    "    location_id_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = 'streamflow',\n",
    "    session_units = 'english',    \n",
    "    measure: str = 'max',\n",
    "    measure_min_allowable = None,\n",
    "    measure_max_allowable = None,\n",
    "    recurrence_flows_df: pd.DataFrame() = None,\n",
    "    high_flow_threshold: str = \"2_0_year_recurrence_flow_17C\",\n",
    ") -> hv.Element:\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get data with geometry\n",
    "    merged_gdf = get_historical_timeseries_chars_with_geom(\n",
    "        gdf = gdf,\n",
    "        geom_id_header = geom_id_header,\n",
    "        source = source,    \n",
    "        location_id_header = location_id_header,\n",
    "        location_id_string = location_id_string, \n",
    "        start_value_time = start_value_time,\n",
    "        end_value_time = end_value_time,\n",
    "        variable_name = variable_name,\n",
    "        units = session_units,        \n",
    "        measure = measure,\n",
    "        measure_min_allowable = measure_min_allowable,\n",
    "        measure_max_allowable = measure_max_allowable,\n",
    "        recurrence_flows_df = recurrence_flows_df,\n",
    "        high_flow_threshold = high_flow_threshold,\n",
    "    )\n",
    "    # get min/max values of the extracted data\n",
    "    measure_min_in_dataset = merged_gdf[measure].min()\n",
    "    measure_max_in_dataset = merged_gdf[measure].max()       \n",
    "\n",
    "    # define data dimensions so plot linkage work\n",
    "    non_measures = [polygon_id_header, location_id_header, 'geometry','units','latitude','longitude','easting','northing']\n",
    "    all_measures = merged_gdf.columns[~merged_gdf.columns.isin(non_measures)].to_list()    \n",
    "             \n",
    "    # define dimensions        \n",
    "    sorted_measures = [measure] + [m for m in all_measures if m!=measure]\n",
    "    vdims = sorted_measures + [location_id_header]\n",
    "    kdims = ['easting','northing']\n",
    "    cols = vdims + kdims + ['latitude','longitude']\n",
    "    \n",
    "    # leave out geometry - easier to work with the data\n",
    "    merged_df = merged_gdf.loc[:,cols]\n",
    "\n",
    "    # if mapping the recurrence interval, sort points so legend appears in order\n",
    "    if measure == 'max_recurr_int': \n",
    "        merged_df = merged_df.sort_values(measure, ascending = False)     \n",
    "    \n",
    "    # declare polygon holoviews object   \n",
    "    points_hv = hv.Points(\n",
    "        merged_df, \n",
    "        kdims = kdims, \n",
    "        vdims = vdims,\n",
    "    )            \n",
    "    return points_hv\n",
    "\n",
    "def get_flow_timeseries_data_selected_point(index):\n",
    "    '''\n",
    "\n",
    "    '''    \n",
    "    print(type(index))\n",
    "    print(index)\n",
    "    \n",
    "    if len(index) > 0 and len(points_dmap.dimensions('value')) > 0:    \n",
    "    \n",
    "#    if len(index) == 0 or len(points_dmap.dimensions('value')) == 0:\n",
    "#         converted_df = pd.DataFrame(np.full((1,2), np.nan), columns = ['value','value_time'], index = ['None'])\n",
    "#         gage_id = \"None\"\n",
    "#         label = f\"usgs_site_code: {gage_id} | NO DATA\" \n",
    "#         val_min = val_max = np.nan\n",
    "        \n",
    "#     else:    \n",
    "        gage_id = points_dmap.dimension_values('usgs_site_code')[index][0]\n",
    "        df = temp_dash_utils.get_historical_timeseries_data(\n",
    "            source = flow_source, \n",
    "            location_id_header = \"usgs_site_code\", \n",
    "            location_id_string = gage_id, \n",
    "            start_value_time = event_dates_slider.value_start,\n",
    "            end_value_time = event_dates_slider.value_end,\n",
    "        )\n",
    "        # convert units\n",
    "        converted_df = temp_data_utils.convert_units(\n",
    "            df, \n",
    "            variable_name = \"precipitation_flux\",\n",
    "            value_columns = [\"value\"],\n",
    "            units = session_units,\n",
    "        )      \n",
    "        val_min = converted_df[\"value\"].min()\n",
    "        val_max = converted_df[\"value\"].max()  \n",
    "        label = f\"usgs_site_code: {gage_id} | max: {val_max} | start_time: {event_dates_slider.value_start}\"         \n",
    "    \n",
    "        ts_curve_hv = hv.Curve(converted_df, \"value_time\", \"value\", label=label)\n",
    "        ts_curve_hv.relabel(label)\n",
    "\n",
    "        # trying to get plot limits to reset, not working\n",
    "        # ts_curve_hv.opts.clear()                                                 \n",
    "        # ts_curve_hv.opts(tools=[\"hover\"], color=\"blue\", ylim=(0, val_max*1.25))\n",
    "        # ts_curve_hv = ts_curve_hv.redim.range(value=(0, val_max)) \n",
    "    \n",
    "        return ts_curve_hv   \n",
    "\n",
    "def get_precip_timeseries_data_selected_point(index):\n",
    "    '''\n",
    "\n",
    "    '''    \n",
    "    if len(index) > 0 and len(points_dmap.dimensions('value')) > 0:      \n",
    "        x = points_dmap.dimension_values('easting')[index][0]\n",
    "        y = points_dmap.dimension_values('northing')[index][0]\n",
    "        pnt = Point(x, y)\n",
    "        catchment = polygons_gdf[(polygons_gdf.contains(pnt) == True)]\n",
    "        catchment_id = catchment[polygon_id_header].iloc[0]\n",
    "\n",
    "        # get the data\n",
    "        df = temp_dash_utils.get_historical_timeseries_data(\n",
    "            source = forcing_source, \n",
    "            location_id_header = \"catchment_id\", \n",
    "            location_id_string = catchment_id, \n",
    "            start_value_time = event_dates_slider.value_start,\n",
    "            end_value_time = event_dates_slider.value_end,\n",
    "        )\n",
    "        # convert units\n",
    "        converted_df = temp_data_utils.convert_units(\n",
    "            df, \n",
    "            variable_name = \"precipitation_flux\",\n",
    "            value_columns = [\"value\"],\n",
    "            units = session_units,\n",
    "        )      \n",
    "        val_min = converted_df[\"value\"].min()\n",
    "        val_max = converted_df[\"value\"].max()  \n",
    "        label = f\"{polygon_id_header}: {catchment_id}\"\n",
    "        \n",
    "    else:\n",
    "        converted_df = pd.DataFrame(np.full((1,2), np.nan), columns = ['value','value_time'], index = ['None'])\n",
    "        catchment_id = \"None\"\n",
    "        label = f\"{polygon_id_header}: {catchment_id} | NO DATA\"\n",
    "        val_min = val_max = np.nan\n",
    "        #ts_curve_hv = hv.Curve(converted_df, label=label)\n",
    "\n",
    "    ts_curve_hv = hv.Curve(converted_df, \"value_time\", \"value\", label=label)\n",
    "\n",
    "    # trying to get plot limits to reset, not working\n",
    "    ts_curve_hv.opts.clear()                                                 \n",
    "    ts_curve_hv.opts(tools=[\"hover\"], color=\"blue\", ylim=(0, val_max*1.25))\n",
    "    ts_curve_hv = ts_curve_hv.redim.range(value=(0, val_max))          \n",
    "    \n",
    "    return ts_curve_hv\n",
    "\n",
    "def get_basemap_gv(opts):\n",
    "    '''\n",
    "    get OSM basemap as geoviews object\n",
    "    '''\n",
    "    tiles = gv.tile_sources.OSM.opts(**opts)\n",
    "    return tiles\n",
    "\n",
    "def get_aggregator(measure):\n",
    "    '''\n",
    "    datashader aggregator function\n",
    "    '''\n",
    "    return ds.mean(measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65d2fa-db6d-4e1a-996c-e826a15b60e0",
   "metadata": {},
   "source": [
    "### Launch the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4a84d-8b78-45e5-af7b-b1155f5d42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(temp_queries)\n",
    "importlib.reload(temp_data_utils)\n",
    "importlib.reload(temp_dash_utils)\n",
    "\n",
    "# Define recurring view options\n",
    "opts = dict(width=600, height=600, show_grid=False)\n",
    "\n",
    "# declare selection widgets - only dates this version\n",
    "event_dates_slider = temp_dash_utils.get_event_date_range_slider(forcing_source)\n",
    "\n",
    "# declare datashader aggregator for precip\n",
    "aggregator = pn.bind(get_aggregator, \"sum\")\n",
    "\n",
    "# bind catchment geoviews to widgets\n",
    "precip_measure = \"sum\"\n",
    "catchments_bind = pn.bind(\n",
    "    get_historical_timeseries_chars_polygons_hv, \n",
    "    gdf = polygons_gdf,\n",
    "    geom_id_header = \"HUC10\",\n",
    "    source = forcing_source,    \n",
    "    location_id_header = \"catchment_id\",\n",
    "    location_id_string = \"all\", \n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,\n",
    "    variable_name = \"precipitation_flux\",\n",
    "    session_units = session_units,    \n",
    "    measure = precip_measure,\n",
    "    measure_min_allowable = 0,\n",
    ")\n",
    "# bind points dataframe to widgets  - !!! Add option to request list of gages within HUC2\n",
    "flow_measure = \"max_recurr_int\"\n",
    "if flow_measure == \"max_recurr_int\":\n",
    "    flow_cmap = temp_dash_utils.get_recurr_colormap()\n",
    "    flow_color_opts = dict(cmap=flow_cmap, legend_position = 'bottom_right')\n",
    "else:\n",
    "    flow_cmap = \"viridis_r\"\n",
    "    flow_color_opts = dict(cmap=flow_cmap, colorbar = True, cnorm = 'log')\n",
    "\n",
    "points_bind = pn.bind(\n",
    "    get_historical_timeseries_chars_points_hv,\n",
    "    gdf = points_gdf,\n",
    "    geom_id_header = \"gage_id\",\n",
    "    source = flow_source,    \n",
    "    location_id_header = \"usgs_site_code\",\n",
    "    location_id_string = \"\", \n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,\n",
    "    variable_name = \"streamflow\",\n",
    "    session_units = session_units,    \n",
    "    measure = flow_measure,\n",
    "    measure_min_allowable = 1,\n",
    "    recurrence_flows_df = recurrence_flows_df,\n",
    "    high_flow_threshold = high_flow_threshold,\n",
    ")\n",
    "\n",
    "# Get rasterized catchments DynamicMap\n",
    "raster_catchments = rasterize(hv.DynamicMap(catchments_bind), \n",
    "                              aggregator=aggregator, precompute=True).opts(**opts, colorbar=True, cmap=temp_dash_utils.get_precip_colormap(), clim=(1, 20))\n",
    "\n",
    "# Get background gage points Element (static)\n",
    "points_background = hv.Points(points_gdf, kdims = ['easting','northing'], vdims = ['gage_id']).opts(color='lightgray', size=2)\n",
    "\n",
    "# Get gage points DynamicMap\n",
    "points_dmap = hv.DynamicMap(points_bind).opts(**opts,\n",
    "    color=hv.dim(flow_measure), size=5, **flow_color_opts)\n",
    "\n",
    "# Define stream source and type to select gage points\n",
    "selection_stream = hv.streams.Selection1D(source=points_dmap)#, index[0])\n",
    " \n",
    "flow_curve_bind = pn.bind(\n",
    "    get_flow_timeseries_data_selected_point,\n",
    "    index=selection_stream.param.index\n",
    ")\n",
    "    \n",
    "# Get flow timeseries DynamicMap\n",
    "#flow_curve_dmap = hv.DynamicMap(get_flow_timeseries_data_selected_point, streams=[selection_stream])\n",
    "\n",
    "# Get precip timeseries DynamicMap\n",
    "precip_curve_dmap = hv.DynamicMap(get_precip_timeseries_data_selected_point, streams=[selection_stream])\n",
    "\n",
    "# Build the Panel layout\n",
    "layout = \\\n",
    "    pn.Column(\n",
    "        pn.Row(\n",
    "            pn.pane.PNG('https://ciroh.ua.edu/wp-content/uploads/2022/08/CIROHLogo_200x200.png', width=100),\n",
    "            pn.pane.Markdown(\n",
    "                \"\"\"\n",
    "                # CIROH Exploratory Evaluation Toolset\n",
    "                ## Post-Event Observed Data Exploration\n",
    "                \"\"\",\n",
    "                width=800\n",
    "            )\n",
    "        ),\n",
    "        event_dates_slider,    \n",
    "        (hv.DynamicMap(get_basemap_gv(opts)) * points_background * raster_catchments) + \n",
    "        (hv.DynamicMap(get_basemap_gv(opts)) * points_background * points_dmap.opts(tools=['hover','tap'])),\n",
    "        flow_curve_bind,\n",
    "        # precip_curve_dmap.opts(width=600) + \n",
    "        # flow_curve_dmap.opts(width=600),\n",
    "        #flow_curve_dmap + \n",
    "        #precip_curve_dmap,\n",
    "    )\n",
    "\n",
    "# launch the layout\n",
    "layout.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf3ae0-7c5a-4afa-af93-a5c45f649b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_hv = get_flow_timeseries_data_selected_point(index=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d8b13-c45a-4158-9a4d-3dcbbefa47ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
