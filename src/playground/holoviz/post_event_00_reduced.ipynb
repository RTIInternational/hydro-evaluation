{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f28e53-63b8-48e1-8017-0c4c85997cae",
   "metadata": {},
   "source": [
    "## Post Event 1 - Explore Event Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436c6c8-f8cb-44be-ad20-7f7886813489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spatialpandas colormap colorcet duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfedb08-6f04-4659-934c-bf6946cfdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "sys.path.insert(0, '../../evaluation/')\n",
    "sys.path.insert(0, '../../evaluation/queries/')\n",
    "\n",
    "from evaluation import utils, config\n",
    "import temp_queries\n",
    "import temp_utils\n",
    "import importlib\n",
    "\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "import colorcet as cc\n",
    "#import hvplot.pandas  \n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import spatialpandas as spd\n",
    "import datashader as ds\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Point\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.operation.datashader import inspect_polygons\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "hv.extension('bokeh', logo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023e81a-d469-47fa-8962-f70f9bdf4820",
   "metadata": {},
   "source": [
    "### Static options (set once at start of session, independent of interactive selections) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131e77b-0534-4304-8f0c-fc97f483007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration (data sources)\n",
    "forcing_source = config.FORCING_ANALYSIS_ASSIM_PARQUET\n",
    "flow_source = config.USGS_PARQUET\n",
    "\n",
    "# TEMPORARY until this info is handled in data model \n",
    "# additional metadata, eventually include other characteristic info - mean upstream slope, %imperv, soils, etc.\n",
    "gage_basin_info_file = pathlib.Path(\"../data/nwm_gage_basin_polygons.feather\")   ## TEMPORARY, these boundaries are not good, have holes, etc.\n",
    "recurrence_flows_file = pathlib.Path(\"../data/nwm_v21_recurrence_flows_17C.nc\")  ## TEMPORARY, until thresholds/recurr_ints are added to data models\n",
    "high_flow_threshold = \"2_0_year_recurrence_flow_17C\"  # TEMPORARY header of 2-yr flows in above recurrence flow file\n",
    "\n",
    "# source and header of MAP polygons corresponding to data in 'forcing_source'\n",
    "polygon_file = pathlib.Path(\"../data/HUC10_Simp005_dd.geojson\")            ## temporary, eventually resolve which layer, how to simplify w/o gaps, \n",
    "geom_location_id_header_polygons = \"HUC10\"                                 ## eventually enable different MAP resolution...\n",
    "\n",
    "# source of HUC2 polygons - for reference only in maps\n",
    "huc2_file = pathlib.Path(\"../data/HUC2_Simp01_RemSPac.geojson\")\n",
    "\n",
    "geom_location_id_header_points = \"gage_id\"\n",
    "data_location_id_header_points = \"usgs_site_code\"\n",
    "data_location_id_header_polygons = \"catchment_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24747089-35d5-4e9a-aa98-2341e6add892",
   "metadata": {},
   "source": [
    "### Read static/independent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5137d9c-0e77-4296-8aed-c1e5428cde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in polygons (if not already in memory - prevent annoying rereading)\n",
    "if not \"polygons_gdf\" in locals():\n",
    "    polygons_gdf = gpd.read_file(polygon_file).to_crs(\"EPSG:3857\")\n",
    "    polygons_gdf = polygons_gdf[[geom_location_id_header_polygons,'geometry']]\n",
    "\n",
    "# read in usgs points (if not already in memory - prevent annoying rereading)\n",
    "if not \"points_gdf\" in locals():\n",
    "    points_gdf = utils.get_usgs_gages()\n",
    "    points_gdf = points_gdf.to_crs(\"EPSG:3857\")\n",
    "    # add easting and northing - helpful for plotting as points on basemap\n",
    "    points_gdf['easting'] = points_gdf.geometry.x\n",
    "    points_gdf['northing'] = points_gdf.geometry.y  \n",
    "\n",
    "    # TEMPORARY:  build crosswalk between points_gdf and polygons_gdf - i.e., for every point, which catchment it falls within, \n",
    "    # if on the border, picks the first one\n",
    "    points_gdf['catchment_id'] = np.nan\n",
    "    for i, point in enumerate(points_gdf['geometry']):\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        pnt = Point(x, y)\n",
    "        catchment_containing_point = polygons_gdf[(polygons_gdf.contains(pnt) == True)]\n",
    "        if not catchment_containing_point.empty:\n",
    "            catchment_id = catchment_containing_point[geom_location_id_header_polygons].iloc[0]  \n",
    "            points_gdf.loc[points_gdf.index[i], 'catchment_id'] = catchment_id    \n",
    "    \n",
    "# read gage_basins, calculate area\n",
    "if not \"gage_basins_gdf\" in locals():\n",
    "    gage_basins = gpd.read_feather(gage_basin_info_file)\n",
    "    gage_basins['area_m2'] = gage_basins.to_crs(\"EPSG:3857\").geometry.area\n",
    "    \n",
    "# read in recurrence flows (if not already in memory - prevent annoying rereading)\n",
    "# recurrence flows are in units of CFS\n",
    "if not \"recurrence_flows_df\" in locals():\n",
    "    recurrence_flows_ds = xr.open_dataset(recurrence_flows_file, engine=\"netcdf4\")\n",
    "    recurrence_flows_df = recurrence_flows_ds.to_dataframe()      \n",
    "    \n",
    "# read in polygons (if not already in memory - prevent annoying rereading)\n",
    "if not \"polygons_gdf\" in locals():\n",
    "    polygons_gdf = gpd.read_file(polygon_file).to_crs(\"EPSG:3857\")\n",
    "    polygons_gdf = polygons_gdf[[geom_location_id_header_polygons,'geometry']]\n",
    "    \n",
    "if not \"huc2_gdf\" in locals():\n",
    "    huc2_gdf_mult = gpd.read_file(huc2_file)\n",
    "    huc2_gdf = gpd.GeoDataFrame()\n",
    "    for i, polys in enumerate(huc2_gdf_mult.geometry):\n",
    "        row = huc2_gdf_mult.loc[[i],['huc2','name','geometry']]\n",
    "        for poly_part in polys.geoms:  \n",
    "            row['geometry'] = poly_part\n",
    "            huc2_gdf = pd.concat([huc2_gdf, row], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13280c64-c8b1-4e91-8cb3-8de6616a7d1a",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54baba-43fd-45fd-969e-cec7f9f64274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_historical_data(   \n",
    "    data_source: str, \n",
    "    data_location_id_header: str = data_location_id_header_points,\n",
    "    data_location_id_like_string: str = \"all\", \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    data_type: str = \"timeseries\",          # 'timeseries' or 'chars'       \n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Run DuckDB query to extract historical timeseries data \n",
    "    or time series characteristics \n",
    "    by region (portion of ID) and value_time range\n",
    "    '''   \n",
    "    # build filters\n",
    "    filters = temp_queries.get_historical_filters(\n",
    "        data_source, \n",
    "        data_location_id_header, \n",
    "        data_location_id_like_string, \n",
    "        start_value_time, \n",
    "        end_value_time\n",
    "    )\n",
    "    #build query\n",
    "    if data_type == 'timeseries':\n",
    "        query = temp_queries.get_historical_timeseries_data_query(\n",
    "            data_source, \n",
    "            data_location_id_header,\n",
    "            filters=filters\n",
    "        )\n",
    "    elif data_type == 'chars':\n",
    "        query = temp_queries.get_historical_timeseries_chars_query(\n",
    "            data_source, \n",
    "            group_by=[data_location_id_header],\n",
    "            order_by=[data_location_id_header],\n",
    "            filters=filters\n",
    "        )        \n",
    "    #run query\n",
    "    df = ddb.query(query).to_df()\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_historical_chars_with_geom(\n",
    "    data_source: str,    \n",
    "    data_location_id_header: str, \n",
    "    data_location_id_like_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = None,      \n",
    "    geom_gdf = gpd.GeoDataFrame(), \n",
    "    geom_id_header: str = None,       \n",
    ") -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    query data chars\n",
    "    merge with geometry\n",
    "    convert units or add recurr flows, if relevant\n",
    "    '''\n",
    "    data_df = query_historical_data(\n",
    "        data_source, \n",
    "        data_location_id_header = data_location_id_header, \n",
    "        data_location_id_like_string = data_location_id_like_string,\n",
    "        start_value_time = start_value_time, \n",
    "        end_value_time = end_value_time,\n",
    "        data_type = \"chars\"\n",
    "    )          \n",
    "    # merge with geodataframe (must do this before adding recurrence flows so have the nwm_feature_id)\n",
    "    data_gdf = temp_utils.merge_df_with_gdf(\n",
    "        geom_gdf, \n",
    "        geom_id_header, \n",
    "        data_df, \n",
    "        data_location_id_header\n",
    "    )                    \n",
    "    # if streamflow, add recurrence flow levels of the peak flows, units currently assumed cfs\n",
    "    if variable_name == \"streamflow\":\n",
    "        keep_measures = ['max']\n",
    "        if not recurrence_flows_df.empty:\n",
    "            data_gdf = temp_utils.add_recurrence_interval(data_gdf, recurrence_flows_df, flow_col_label = \"max\")\n",
    "            keep_measures = keep_measures + ['max_recurr_int']        \n",
    "            \n",
    "    # if precip, convert to inches/hr, currently in mm/s - for all calculated values/measures returned by query\n",
    "    all_measures = data_df.columns[~data_df.columns.isin([data_location_id_header, 'units'])].to_list() \n",
    "    if variable_name == \"precipitation_flux\":\n",
    "        keep_measures = ['sum']        \n",
    "        for col in all_measures:\n",
    "            data_gdf[col] = round(data_gdf[col]*60*60, 2)    \n",
    "            data_gdf[col] = data_gdf[col] / 25.4     \n",
    "            \n",
    "    # reduce columns as work around to custom hover tool not working in dynamicmap\n",
    "    keep_cols = [data_location_id_header,'geometry','units'] + keep_measures\n",
    "    if data_gdf.geom_type.values[0] == 'Point':\n",
    "        keep_cols = keep_cols + ['latitude','longitude','easting','northing']\n",
    "    data_gdf = data_gdf.loc[:,keep_cols]   \n",
    "        \n",
    "    return data_gdf\n",
    "\n",
    "\n",
    "def get_historical_timeseries(\n",
    "    data_source: str,    \n",
    "    data_location_id_header: str, \n",
    "    data_location_id_like_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = None,          \n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    query data\n",
    "    convert/transform/process data\n",
    "    '''\n",
    "    data_df = query_historical_data(\n",
    "        data_source, \n",
    "        data_location_id_header = data_location_id_header, \n",
    "        data_location_id_like_string = data_location_id_like_string,\n",
    "        start_value_time = start_value_time, \n",
    "        end_value_time = end_value_time,\n",
    "        data_type = \"timeseries\"\n",
    "    )          \n",
    "    # if precip, convert values to inches/hr, currently in mm/s\n",
    "    if variable_name == \"precipitation_flux\":\n",
    "        data_df['value'] = round(data_df['value']*60*60, 2)    \n",
    "        data_df['value'] = data_df['value'] / 25.4      \n",
    "        data_df['value_cum'] = data_df['value'].cumsum()\n",
    "        \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb0399-cdd1-4674-b7a5-0c45d486ad1e",
   "metadata": {},
   "source": [
    "### Holoviews object definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895d710-90e8-4eeb-9b0b-b6aba27542aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_chars_as_geo_element(\n",
    "    data_source: str,     \n",
    "    data_location_id_header: str, \n",
    "    data_location_id_like_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = None,       \n",
    "    geom_gdf = gpd.GeoDataFrame(), \n",
    "    geom_id_header: str = None,     \n",
    "    measure: str = None,\n",
    "    measure_min_requested = None,\n",
    "    measure_max_requested = None,\n",
    ") -> hv.Element:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # get data with geometry\n",
    "    data_gdf = get_historical_chars_with_geom(\n",
    "        data_source = data_source,\n",
    "        data_location_id_header = data_location_id_header,\n",
    "        data_location_id_like_string = data_location_id_like_string, \n",
    "        start_value_time = start_value_time,\n",
    "        end_value_time = end_value_time,\n",
    "        variable_name = variable_name,        \n",
    "        geom_gdf = geom_gdf,\n",
    "        geom_id_header = geom_id_header,               \n",
    "    )\n",
    "    # subset data based on requested min/max (if any defined, e.g., only > 0 or other threshold)\n",
    "    if measure_min_requested:\n",
    "        data_gdf = data_gdf[data_gdf[measure] >= measure_min_requested]\n",
    "    if measure_max_requested:\n",
    "        data_gdf = data_gdf[data_gdf[measure] <= measure_max_requested]\n",
    "    \n",
    "    # find the actual min/max values of the extracted data for rescaling plots\n",
    "    measure_min_in_dataset = data_gdf[measure].min()\n",
    "    measure_max_in_dataset = data_gdf[measure].max()    \n",
    "          \n",
    "    #convert to spatialpandas object (required for inspect polygons function)\n",
    "    data_sdf = spd.GeoDataFrame(data_gdf)   \n",
    "    \n",
    "    # check geometry type\n",
    "    geom_type = data_gdf.geometry.type.iloc[0]\n",
    "    \n",
    "    if geom_type == 'Polygon':    \n",
    "        \n",
    "        # declare polygon geoviews object           \n",
    "        label = f\"Mean Areal Precip | {start_value_time} | {end_value_time}\"\n",
    "        map_element_hv = gv.Polygons(\n",
    "            data_sdf,\n",
    "            crs=ccrs.GOOGLE_MERCATOR, \n",
    "            vdims=[measure, data_location_id_header],\n",
    "            label = label,\n",
    "        )    \n",
    "        \n",
    "    elif geom_type == 'Point':      \n",
    "        # define data dimensions - more complex for points so plot linkages work\n",
    "        non_measures = [geom_location_id_header_polygons, data_location_id_header, \n",
    "                        'geometry','units','latitude','longitude','easting','northing']\n",
    "        all_measures = data_sdf.columns[~data_gdf.columns.isin(non_measures)].to_list()    \n",
    "\n",
    "        # define dimensions        \n",
    "        sorted_measures = [measure] + [m for m in all_measures if m!=measure]\n",
    "        vdims = sorted_measures + [data_location_id_header]\n",
    "        kdims = ['easting','northing']\n",
    "        all_cols_except_geom = vdims + kdims + ['latitude','longitude']\n",
    "\n",
    "        # leave out geometry - easier to work with the data\n",
    "        data_df = data_sdf.loc[:,all_cols_except_geom]\n",
    "\n",
    "        # if mapping the recurrence interval, sort points so legend appears in order\n",
    "        if measure == 'max_recurr_int': \n",
    "            data_df = data_df.sort_values(measure, ascending = False)     \n",
    "\n",
    "        # declare points holoviews object   \n",
    "        label = f\"{measure} | {start_value_time} | {end_value_time}\"\n",
    "        map_element_hv = hv.Points(\n",
    "            data_df, \n",
    "            kdims = kdims, \n",
    "            vdims = vdims,\n",
    "            label = label,\n",
    "        )\n",
    "        map_element_hv.relabel(f\"{measure} | {start_value_time} | {end_value_time}\")\n",
    "        \n",
    "        tooltips = [('ID', '@usgs_site_code'),('Max Flow (cfs)', '@max')]\n",
    "        hover = HoverTool(tooltips=tooltips)            \n",
    "        map_element_hv.opts(tools=[hover])\n",
    "\n",
    "    # reset the data range based on data in the current sample\n",
    "    map_element_hv.redim.range(**{f\"{measure}\": (measure_min_in_dataset, measure_max_in_dataset)})  \n",
    "    map_element_hv.relabel(label)\n",
    "        \n",
    "    return map_element_hv    \n",
    "\n",
    "\n",
    "def get_historical_timeseries_as_ts_element(\n",
    "    index: List[int],\n",
    "    points_dmap: hv.DynamicMap,\n",
    "    variable_name: str = \"streamflow\", \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    element_type = \"curve\",\n",
    "    opts = {},\n",
    "):\n",
    "    '''\n",
    "\n",
    "    '''    \n",
    "    if len(index) > 0 and len(points_dmap.dimensions('value')) > 0:    \n",
    "     \n",
    "        point_id = points_dmap.dimension_values(data_location_id_header_points)[index][0]\n",
    "        \n",
    "        if variable_name == \"precipitation_flux\":\n",
    "            polygon_id = points_gdf.loc[points_gdf[geom_location_id_header_points] == point_id, 'catchment_id'].iloc[0]  \n",
    "            df = get_historical_timeseries(\n",
    "                data_source = forcing_source,\n",
    "                data_location_id_header = data_location_id_header_polygons, \n",
    "                data_location_id_like_string = polygon_id, \n",
    "                start_value_time = event_dates_slider.value_start,\n",
    "                end_value_time = event_dates_slider.value_end,\n",
    "                variable_name = variable_name,\n",
    "            )            \n",
    "            label = f\"{geom_location_id_header_polygons}: {polygon_id}\"  \n",
    "            \n",
    "            if element_type == \"curve\":\n",
    "                ts_element_hv = hv.Curve(df, (\"value_time\", \"Date\"), (\"value_cum\", \"Cum. Precip (in)\"), label=label)\n",
    "            elif element_type == \"bars\":\n",
    "                ts_element_hv = hv.Bars(df, (\"value_time\", \"Date\"), (\"value_cum\", \"Cum. Precip (in)\"), label=label)\n",
    "                \n",
    "            ts_element_hv.relabel(label)\n",
    "            ts_element_hv.opts(**opts, xaxis = False, color=\"cyan\")            \n",
    "            \n",
    "        elif variable_name == \"streamflow\":\n",
    "            \n",
    "            df = get_historical_timeseries(\n",
    "                data_source = flow_source,\n",
    "                data_location_id_header = data_location_id_header_points, \n",
    "                data_location_id_like_string = point_id, \n",
    "                start_value_time = event_dates_slider.value_start,\n",
    "                end_value_time = event_dates_slider.value_end,\n",
    "                variable_name = variable_name,\n",
    "            ) \n",
    "            label = f\"{geom_location_id_header_points}: {point_id}\"\n",
    "            ts_element_hv = hv.Curve(df, (\"value_time\", \"Date\"), (\"value\", \"Flow (cfs)\"), label=label)\n",
    "            ts_element_hv.relabel(label)\n",
    "            ts_element_hv.opts(**opts, color=\"blue\")\n",
    "    \n",
    "        return ts_element_hv      \n",
    "\n",
    "            \n",
    "def get_aggregator(measure):\n",
    "    '''\n",
    "    datashader aggregator function\n",
    "    '''\n",
    "    return ds.mean(measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65d2fa-db6d-4e1a-996c-e826a15b60e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Launch the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4a84d-8b78-45e5-af7b-b1155f5d42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(temp_queries)\n",
    "importlib.reload(temp_utils)\n",
    "pn.extension(sizing_mode='fixed') #'stretch_width')\n",
    "\n",
    "# declare selection widgets - only dates this version\n",
    "event_dates_slider = temp_utils.get_event_date_range_slider([forcing_source, flow_source])\n",
    "\n",
    "# declare datashader aggregator\n",
    "aggregator = pn.bind(get_aggregator, \"sum\")\n",
    "\n",
    "# bind catchment geoviews to widgets (pulling CONUS wide, obs data does not warrant selection by HUC2)\n",
    "catchments_bind = pn.bind(\n",
    "    get_historical_chars_as_geo_element, \n",
    "    data_source = forcing_source,   \n",
    "    data_location_id_header = data_location_id_header_polygons,    \n",
    "    data_location_id_like_string = \"all\",     \n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,    \n",
    "    variable_name = \"precipitation_flux\",  \n",
    "    geom_gdf = polygons_gdf,\n",
    "    geom_id_header = geom_location_id_header_polygons, \n",
    "    measure = \"sum\",\n",
    "    measure_min_requested = 0,\n",
    ")\n",
    "# bind points dataframe to widgets  - !!! Add option to request list of gages within HUC2   \n",
    "points_bind = pn.bind(\n",
    "    get_historical_chars_as_geo_element,\n",
    "    data_source = flow_source,\n",
    "    data_location_id_header = data_location_id_header_points,\n",
    "    data_location_id_like_string = \"all\",     \n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,     \n",
    "    variable_name = \"streamflow\",     \n",
    "    geom_gdf = points_gdf,\n",
    "    geom_id_header = geom_location_id_header_points,\n",
    "    measure = \"max_recurr_int\",\n",
    "    measure_min_requested = 1,\n",
    ")\n",
    "\n",
    "# Build background (static) map Elements - background tiles and all gage points for reference rasterized catchments DynamicMap\n",
    "tiles_background = gv.tile_sources.OSM\n",
    "points_background = hv.Points(points_gdf, kdims = ['easting','northing'], vdims = ['gage_id'])\n",
    "\n",
    "# Build dynamic (changing) DynamicMaps - rasterized catchments (left map) and current high flow gage points (right map)\n",
    "raster_catchments = rasterize(hv.DynamicMap(catchments_bind), aggregator=aggregator, precompute=True)\n",
    "points_dmap = hv.DynamicMap(points_bind)\n",
    "\n",
    "# Define stream source as selected gage points\n",
    "selection_stream = hv.streams.Selection1D(source=points_dmap, index=[0])\n",
    "    \n",
    "curve_opts = dict(width=600, height=200, toolbar = None, tools=[\"hover\"])\n",
    "    \n",
    "flow_curve_bind = pn.bind(\n",
    "    get_historical_timeseries_as_ts_element,\n",
    "    index=selection_stream.param.index,\n",
    "    points_dmap = points_dmap,\n",
    "    variable_name=\"streamflow\",\n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,   \n",
    "    element_type = \"curve\",\n",
    "    opts = curve_opts,\n",
    ")\n",
    "    \n",
    "precip_bars_bind = pn.bind(\n",
    "    get_historical_timeseries_as_ts_element,\n",
    "    index=selection_stream.param.index,\n",
    "    points_dmap = points_dmap,\n",
    "    variable_name=\"precipitation_flux\",\n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,\n",
    "    element_type = \"curve\",\n",
    "    opts = curve_opts,\n",
    ")\n",
    "\n",
    "# various plotting options\n",
    "map_opts = dict(width=600, height=400, show_grid=False, xaxis = None, yaxis = None)\n",
    "\n",
    "tiles_background.opts(**map_opts)\n",
    "raster_catchments.opts(**map_opts, colorbar=True, cmap=temp_utils.get_precip_colormap(), clim=(1, 20), toolbar = 'right')\n",
    "points_background.opts(**map_opts, color='lightgray', size=2, toolbar = 'right')\n",
    "points_dmap.opts(**map_opts, tools=['hover','tap'], color=hv.dim(\"max_recurr_int\"), cmap=temp_utils.get_recurr_colormap(), \n",
    "                 size = 5, legend_position = 'bottom_right', toolbar = 'right')\n",
    "\n",
    "\n",
    "header = pn.Row(\n",
    "            pn.pane.PNG('https://ciroh.ua.edu/wp-content/uploads/2022/08/CIROHLogo_200x200.png', width=100),\n",
    "            pn.pane.Markdown(\n",
    "                \"\"\"\n",
    "                # CIROH Exploratory Evaluation Toolset\n",
    "                ## Post-Event Observed Data Exploration\n",
    "                \"\"\",\n",
    "                width=800\n",
    "            )\n",
    ")\n",
    "# Build the Panel layout\n",
    "layout = \\\n",
    "    pn.Column(\n",
    "        header,\n",
    "        event_dates_slider,    \n",
    "        (tiles_background * points_background * raster_catchments) + \n",
    "        (tiles_background * points_background * points_dmap),\n",
    "        pn.Row(precip_bars_bind, flow_curve_bind),\n",
    "    )\n",
    "\n",
    "# launch the layout\n",
    "layout.servable()\n",
    "\n",
    "\n",
    "## alter non-select alpha, add border on selected?\n",
    "## accum precip to daily, add bar chart\n",
    "## axis labels\n",
    "## flow hover - worked around for now\n",
    "## dot size on zoom in - too small to see\n",
    "## switch to max/ave flow\n",
    "## prevent ts plots from disappearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47812f1a-2c9d-4a6e-8305-c4b3ed4083ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_historical_timeseries_as_ts_element(\n",
    "    index=selection_stream.param.index,\n",
    "    points_dmap = points_dmap,\n",
    "    variable_name=\"precipitation_flux\",\n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,\n",
    "    element_type = \"curve\",\n",
    "    opts = curve_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a07cfe-00f7-4210-aeef-a574ee893b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
