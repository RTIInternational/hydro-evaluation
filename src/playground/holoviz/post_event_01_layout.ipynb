{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f28e53-63b8-48e1-8017-0c4c85997cae",
   "metadata": {},
   "source": [
    "## Post Event 1 - Explore Event Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436c6c8-f8cb-44be-ad20-7f7886813489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spatialpandas colormap colorcet duckdb streamz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfedb08-6f04-4659-934c-bf6946cfdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "sys.path.insert(0, '../../evaluation/')\n",
    "sys.path.insert(0, '../../evaluation/queries/')\n",
    "\n",
    "#import post_event_dashboard_1 as db1\n",
    "import temp_queries\n",
    "from evaluation import utils, config\n",
    "import importlib\n",
    "\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "import colorcet as cc \n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import spatialpandas as spd\n",
    "import datashader as ds\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Point\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from bokeh.models import HoverTool, Range1d\n",
    "\n",
    "hv.extension('bokeh', logo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023e81a-d469-47fa-8962-f70f9bdf4820",
   "metadata": {},
   "source": [
    "### Define static options (independent of interactive selections) \n",
    "\n",
    "#### All structures temporary until data models are finalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131e77b-0534-4304-8f0c-fc97f483007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference polygons \n",
    "polygon_info = dict(\n",
    "    huc2_file = pathlib.Path(\"../data/HUC2_Simp01_RemSPac.geojson\"), # TEMPORARY, gaps/holes\n",
    "    huc2_header = \"huc2\",\n",
    "    huc10_file = pathlib.Path(\"../data/HUC10_Simp005_dd.geojson\"),   # TEMPORARY, gaps/holes\n",
    "    huc10_header = \"HUC10\",\n",
    ")\n",
    "\n",
    "forcing_info = dict(\n",
    "    source = config.FORCING_ANALYSIS_ASSIM_PARQUET,\n",
    "    data_location_id_header = \"catchment_id\",\n",
    "    geom_location_id_header = polygon_info['huc10_header'],\n",
    "    geom_file = polygon_info['huc10_file'],\n",
    ")\n",
    "\n",
    "streamflow_info = dict(\n",
    "    source = config.USGS_PARQUET,\n",
    "    data_location_id_header = \"usgs_site_code\",\n",
    "    geom_location_id_header = \"gage_id\",\n",
    "    gage_basins_file = pathlib.Path(\"../data/gage_basins.geojson\")   # Multipolygons, some old, some holes, need to get outer boundary (explode, concave hull)\n",
    "    #recurrence_flows_file = pathlib.Path(\"../data/nwm_v21_recurrence_flows_17C.nc\"),  # TEMPORARY, until thresholds/recurr_ints are added to data models\n",
    "    #high_flow_threshold = \"2_0_year_recurrence_flow_17C\",                             # TEMPORARY header of 2-yr flows in above recurrence flow file\n",
    ")\n",
    "# eventually include other characteristic info - mean upstream slope, %imperv, soils, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24747089-35d5-4e9a-aa98-2341e6add892",
   "metadata": {},
   "source": [
    "### Read static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc85515-39be-406a-a111-e51275882b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(db1)\n",
    "\n",
    "# read static data if not already in memory (prevent annoying rereading)\n",
    "\n",
    "# read in polygons associated with MAP data\n",
    "if not \"polygons_gdf\" in locals():\n",
    "    print('Reading polygons...')\n",
    "    polygons_gdf = gpd.read_file(forcing_info['geom_file']).to_crs(\"EPSG:3857\")\n",
    "    polygons_gdf = polygons_gdf[[forcing_info['geom_location_id_header'],'geometry']]\n",
    "    \n",
    "# read in usgs points\n",
    "if not \"points_gdf\" in locals():\n",
    "    print('Reading usgs points...')\n",
    "    points_gdf = db1.read_points(streamflow_info)\n",
    "    #points_gdf = gpd.read_parquet(streamflow_info['gage_points_file'])\n",
    "    \n",
    "# TEMPORARY:  build crosswalk between points_gdf and polygons_gdf - i.e., for every point, which catchment it falls within, \n",
    "# if on the border, picks the first one\n",
    "if not 'catchment_id' in points_gdf.columns:\n",
    "    print('Building catchment-gage point crosswalk...')\n",
    "    points_gdf['catchment_id'] = np.nan\n",
    "    for i, point in enumerate(points_gdf['geometry']):\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        pnt = Point(x, y)\n",
    "        catchment_containing_point = polygons_gdf[(polygons_gdf.contains(pnt) == True)]\n",
    "        if not catchment_containing_point.empty:\n",
    "            catchment_id = catchment_containing_point[forcing_info['geom_location_id_header']].iloc[0]\n",
    "            points_gdf.loc[points_gdf.index[i], 'catchment_id'] = catchment_id    \n",
    "            \n",
    "# TEMPORARY read gage_basins, add area to points_gdf\n",
    "if not \"gage_basins_gdf\" in locals():\n",
    "    print('Reading gage basins...')\n",
    "    gage_basins_gdf = gpd.read_file(streamflow_info['gage_basins_file']).set_index('usgs_site_code')\n",
    "    gage_basins_gdf['AREA_KM2'] = gage_basins_gdf['AREA_FT2'] / (3.28**2) / (1000**2)\n",
    "    points_gdf['upstr_area_km2'] = np.nan\n",
    "    ind_w_upstr_area = points_gdf[points_gdf['gage_id'].isin(gage_basins_gdf.index)].index\n",
    "    points_gdf.loc[ind_w_upstr_area,'upstr_area_km2'] = \\\n",
    "        gage_basins_gdf.loc[points_gdf.loc[ind_w_upstr_area,'gage_id'],'AREA_KM2'].to_numpy()\n",
    "    \n",
    "# TEMPORARY read in recurrence flows (if not already in memory - prevent annoying rereading)\n",
    "# recurrence flows are in units of CFS\n",
    "# if not \"recurrence_flows_df\" in locals():\n",
    "#     print('Reading recurrence flows...')\n",
    "#     recurrence_flows_ds = xr.open_dataset(streamflow_info['recurrence_flows_file'], engine=\"netcdf4\")\n",
    "#     recurrence_flows_df = recurrence_flows_ds.to_dataframe()      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb0399-cdd1-4674-b7a5-0c45d486ad1e",
   "metadata": {},
   "source": [
    "### Holoviews object definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895d710-90e8-4eeb-9b0b-b6aba27542aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_chars_geo_element(\n",
    "    data_info: dict,\n",
    "    data_location_id_like_string: str, \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,\n",
    "    variable_name = None,       \n",
    "    geom_gdf = gpd.GeoDataFrame(),    \n",
    "    measure: str = None,\n",
    "    measure_min_requested = None,\n",
    "    measure_max_requested = None,\n",
    "    opts = {},\n",
    ") -> hv.Element:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # get data with geometry\n",
    "    data_gdf = db1.get_historical_chars_with_geom(\n",
    "        data_info = data_info,\n",
    "        data_location_id_like_string = data_location_id_like_string, \n",
    "        start_value_time = start_value_time,\n",
    "        end_value_time = end_value_time,\n",
    "        variable_name = variable_name,        \n",
    "        geom_gdf = geom_gdf,               \n",
    "    )   \n",
    "    # if mapping the recurrence interval, add recurrence values\n",
    "    if measure == 'max_recurr_int':\n",
    "        data_gdf = db1.add_recurrence_interval(data_gdf, recurrence_flows_df, flow_col_label = \"max\")   \n",
    "    if measure == 'max_in_hr':\n",
    "        data_gdf = data_gdf[data_gdf['upstr_area_km2'].notnull()].copy()\n",
    "        data_gdf['max_in_hr'] = data_gdf['max'] / (data_gdf['upstr_area_km2']*(1000**2)*(3.28**2)) * 12 * 3600\n",
    "    \n",
    "    # subset data based on requested min/max (if any defined, e.g., only > 0 or other threshold)\n",
    "    if measure_min_requested:\n",
    "        data_gdf = data_gdf[data_gdf[measure] >= measure_min_requested]\n",
    "    if measure_max_requested:\n",
    "        data_gdf = data_gdf[data_gdf[measure] <= measure_max_requested]\n",
    "    \n",
    "    # find the actual min/max values of the extracted data for rescaling plots\n",
    "    measure_min_in_dataset = data_gdf[measure].min()\n",
    "    measure_max_in_dataset = data_gdf[measure].max()    \n",
    "          \n",
    "    #convert to spatialpandas object (required for inspect polygons function)\n",
    "    data_sdf = spd.GeoDataFrame(data_gdf)   \n",
    "    \n",
    "    # check geometry type\n",
    "    geom_type = data_gdf.geometry.type.iloc[0]\n",
    "    \n",
    "    if geom_type == 'Polygon':    \n",
    "        \n",
    "        # declare polygon geoviews object           \n",
    "        #label = f\"Mean Areal Precip | {start_value_time} | {end_value_time}\"\n",
    "        map_element_hv = gv.Polygons(\n",
    "            data_sdf,\n",
    "            crs=ccrs.GOOGLE_MERCATOR, \n",
    "            vdims=[measure, data_info['data_location_id_header']],\n",
    "            #label = label,\n",
    "        )  \n",
    "        map_element_hv.opts(**opts)           \n",
    "        \n",
    "    elif geom_type == 'Point':      \n",
    "        # define data dimensions - more complex for points so plot linkages work\n",
    "        non_measures = [data_info['geom_location_id_header'], data_info['data_location_id_header'], \n",
    "                        'geometry','units','latitude','longitude','easting','northing','nwm_feature_id','upstr_area_km2']\n",
    "        # custom hover not working, limit to 'measure' column only - possibly make this an argument\n",
    "        show_all_columns = False\n",
    "        if show_all_columns:\n",
    "            all_measures = data_sdf.columns[~data_gdf.columns.isin(non_measures)].to_list()    \n",
    "        else:\n",
    "            all_measures = [measure]\n",
    "\n",
    "        # define dimensions        \n",
    "        sorted_measures = [measure] + [m for m in all_measures if m!=measure]\n",
    "        vdims = sorted_measures + [data_info['data_location_id_header'], 'upstr_area_km2']\n",
    "        kdims = ['easting','northing']\n",
    "        all_cols_except_geom = vdims + kdims + ['latitude','longitude']\n",
    "\n",
    "        # leave out geometry - easier to work with the data\n",
    "        data_df = data_sdf.loc[:,all_cols_except_geom]\n",
    "\n",
    "        # if mapping the recurrence interval, add recurrence values\n",
    "        # and sort points so legend appears in order\n",
    "        if measure == 'max_recurr_int':        \n",
    "            data_df = data_df.sort_values(measure, ascending = False)     \n",
    "\n",
    "        # declare points holoviews object   \n",
    "        label = f\"{measure} | {start_value_time} | {end_value_time}\"\n",
    "        map_element_hv = hv.Points(\n",
    "            data_df, \n",
    "            kdims = kdims, \n",
    "            vdims = vdims,\n",
    "            #label = label,\n",
    "        )\n",
    "        map_element_hv.opts(**opts, show_legend=False)        \n",
    "        \n",
    "        # tooltips = [('ID', '@usgs_site_code'),('Max Flow (cfs)', '@max')]  ###  custom tooltips is not working\n",
    "        # hover = HoverTool(tooltips=tooltips)            \n",
    "        # map_element_hv.opts(tools=[hover])\n",
    "\n",
    "    # reset the data range based on data in the current sample\n",
    "    map_element_hv.redim.range(**{f\"{measure}\": (measure_min_in_dataset, measure_max_in_dataset)})  \n",
    "    #map_element_hv.relabel(label)\n",
    "\n",
    "    return map_element_hv    \n",
    "\n",
    "def get_catchment_polygon_hv(\n",
    "    index: List[int],\n",
    "    points_dmap: hv.DynamicMap,     \n",
    "    points_info: dict = {},\n",
    "    polygons_info: dict = {},\n",
    "    points_gdf: gpd.GeoDataFrame() = None,\n",
    "    polygons_gdf: gpd.GeoDataFrame() = None,   \n",
    "    opts = {},\n",
    ") -> hv.Element:\n",
    "    \n",
    "    if len(index) > 0 and len(points_dmap.dimensions('value')) > 0:    \n",
    "        point_id = points_dmap.dimension_values(points_info['data_location_id_header'])[index][0]\n",
    "    else:\n",
    "        point_id = points_dmap.dimension_values(points_info['data_location_id_header'])[0]\n",
    "        opts = dict(opts, alpha=0)\n",
    "        \n",
    "    polygon_id = points_gdf.loc[points_gdf[points_info['geom_location_id_header']] == point_id, 'catchment_id'].iloc[0]  \n",
    "    selected_polygon = polygons_gdf.loc[polygons_gdf[polygons_info['geom_location_id_header']]==polygon_id,:]\n",
    "\n",
    "    polygon_hv = gv.Polygons(selected_polygon, crs=ccrs.GOOGLE_MERCATOR, vdims=[forcing_info['geom_location_id_header']])\n",
    "    polygon_hv.opts(**opts)     \n",
    "        \n",
    "    return polygon_hv\n",
    "\n",
    "\n",
    "def get_historical_timeseries_ts_element(\n",
    "    index: List[int],\n",
    "    points_dmap: hv.DynamicMap,    \n",
    "    points_info: dict = {},\n",
    "    polygons_info: dict = {},\n",
    "    points_gdf: gpd.GeoDataFrame() = None,\n",
    "    polygons_gdf: gpd.GeoDataFrame() = None,\n",
    "    variable_name: str = \"streamflow\", \n",
    "    start_value_time: pd.Timestamp = None,\n",
    "    end_value_time: pd.Timestamp = None,   \n",
    "    element_type = \"curve\",\n",
    "    opts = {},\n",
    "):\n",
    "    '''\n",
    "\n",
    "    '''    \n",
    "    if len(index) > 0 and len(points_dmap.dimensions('value')) > 0:    \n",
    "     \n",
    "        point_id = points_dmap.dimension_values(points_info['data_location_id_header'])[index][0]\n",
    "        \n",
    "        if variable_name == \"precipitation_flux\":\n",
    "            polygon_id = points_gdf.loc[points_gdf[points_info['geom_location_id_header']] == point_id, 'catchment_id'].iloc[0]  \n",
    "            title = f\"{polygons_info['geom_location_id_header']}: {polygon_id} (Contains Gage: {point_id})\"                 \n",
    "            df = db1.get_historical_timeseries(\n",
    "                data_info = polygons_info,\n",
    "                data_location_id_like_string = polygon_id, \n",
    "                start_value_time = event_dates_slider.value_start,\n",
    "                end_value_time = event_dates_slider.value_end,\n",
    "                variable_name = variable_name,\n",
    "            )            \n",
    "            df['value_time_str'] = df['value_time'].dt.strftime('%Y-%m-%d-%H')\n",
    "            ymax_bars = max(df['value'].max()*1.1,1)\n",
    "            ymax_curve = max(df['value_cum'].max()*1.1,1)\n",
    "            df = df.rename(columns = {'value_cum':'Cumulative (in)'})  # work around to get correct label on secondary axis\n",
    "            t = start_value_time + (end_value_time - start_value_time)*0.01\n",
    "            text_x = t.replace(second=0, microsecond=0, minute=0).strftime('%Y-%m-%d-%H')\n",
    "            text_y = ymax_bars*0.9    \n",
    "\n",
    "            bars = hv.Bars(df, kdims = [('value_time_str','Date')], vdims = [('value', 'Precip Rate (in/hr)')])\n",
    "            curve = hv.Curve(df, kdims = [(\"value_time_str\", \"Date\")], vdims = [('Cumulative (in)', 'Precip (in)')])\n",
    "            text = hv.Text(text_x, text_y, title).opts(text_align='left', text_font_size='10pt', text_color='#57504d', text_font_style='bold')\n",
    "        \n",
    "            bars.opts(**opts, fill_color = 'blue', line_color = None, ylim=(0, ymax_bars))\n",
    "            curve.opts(**opts, color='orange', hooks=[db1.plot_secondary_bars_curve])\n",
    "            \n",
    "            ts_element_hv = (bars * curve * text).opts(show_title=False)  ##  must control ylim of secondary axis in hook function!\n",
    "            \n",
    "            \n",
    "            #ts_element_hv = bars * curve\n",
    "            #ts_element_hv.relabel(label)\n",
    "            \n",
    "        elif variable_name == \"streamflow\":\n",
    "            title = f\"Gage ID: {point_id}\"\n",
    "            df = db1.get_historical_timeseries(\n",
    "                data_info = points_info,\n",
    "                data_location_id_like_string = point_id, \n",
    "                start_value_time = event_dates_slider.value_start,\n",
    "                end_value_time = event_dates_slider.value_end,\n",
    "                variable_name = variable_name,\n",
    "            )           \n",
    "            \n",
    "            upstr_area_km2 = points_dmap.dimension_values('upstr_area_km2')[index][0]\n",
    "            upstr_area_ft2 = upstr_area_km2*(1000**2)*(3.28**2)\n",
    "            df[\"Norm. Flow (in/hr)\"] = df['value']/upstr_area_ft2*12*3600\n",
    "            ymax = max(df['value'].max()*1.1,1)\n",
    "            t = start_value_time + (end_value_time - start_value_time)*0.01\n",
    "            text_x = t.replace(second=0, microsecond=0, minute=0)\n",
    "            text_y = ymax*0.9       \n",
    "\n",
    "            text = hv.Text(text_x, text_y, title).opts(text_align='left', text_font_size='10pt', text_color='#57504d', text_font_style='bold')\n",
    "            curve_cfs = hv.Curve(df, (\"value_time\", \"Date\"), (\"value\", \"Flow (ft3/s)\")) \n",
    "            curve_in_hr = hv.Curve(df, (\"value_time\", \"Date\"), (\"Norm. Flow (in/hr)\", \"Norm. Flow (in/hr)\")) \n",
    "\n",
    "            curve_cfs.opts(**opts, color='blue', ylim=(0, ymax))\n",
    "            curve_in_hr.opts(**opts, color='orange', alpha=0, hooks=[db1.plot_secondary_curve_curve])\n",
    "\n",
    "            ts_element_hv = (curve_cfs * curve_in_hr * text).opts(show_title=False)  \n",
    "            \n",
    "            \n",
    "    else:        \n",
    "        df = pd.DataFrame([[0,0],[1,0]], columns = ['Date','value'])\n",
    "        label = \"Nothing Selected\"\n",
    "        curve = hv.Curve(df, \"Date\", \"value\").opts(**opts)\n",
    "        text = hv.Text(0.01, 0.9, \"No Selection\").opts(text_align='left', text_font_size='10pt', text_color='#57504d', text_font_style='bold')\n",
    "        ts_element_hv = (curve * text).opts(show_title=False)\n",
    "            \n",
    "    return ts_element_hv      \n",
    "\n",
    "            \n",
    "def get_aggregator(measure):\n",
    "    '''\n",
    "    datashader aggregator function\n",
    "    '''\n",
    "    return ds.mean(measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65d2fa-db6d-4e1a-996c-e826a15b60e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Launch the Dashboard"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abf24b3b-3a9a-473e-a7d0-03caf31e2dd9",
   "metadata": {},
   "source": [
    "importlib.reload(temp_queries)\n",
    "importlib.reload(db1)\n",
    "pn.extension(sizing_mode='scale_both')\n",
    "\n",
    "\n",
    "flow_measure = \"max_in_hr\"  #\"max_recurr_int\"\n",
    "flow_map_title = \"Normalized Event Peak Flow (in/hr)\"\n",
    "precip_map_title = \"Total Precipitation (in)\"\n",
    "\n",
    "\n",
    "######### Common plotting options\n",
    "\n",
    "map_opts = dict(show_grid=False, xaxis = None, yaxis = None, width=550, height=350)\n",
    "\n",
    "if flow_measure == \"max_recurr_int\":\n",
    "    points_cmap_opts = dict(cmap=db1.get_recurr_colormap(), legend_position='bottom_right')\n",
    "else:\n",
    "    points_cmap_opts = dict(cmap=cc.CET_L8[::-1], cnorm='eq_hist', colorbar=True) \n",
    "    \n",
    "curve_opts = dict(toolbar = None, tools=[\"hover\"], show_title = False, width=1050)\n",
    "\n",
    "    \n",
    "######### Build components for the dashboard\n",
    "\n",
    "# declare datashader aggregator\n",
    "aggregator = pn.bind(get_aggregator, \"sum\")\n",
    "\n",
    "# declare selection widgets - only dates this version\n",
    "event_dates_slider = db1.get_event_date_range_slider([forcing_info['source'], streamflow_info['source']], dict(width=600))\n",
    "    \n",
    "# Build background (static) map Elements - background tiles and all gage points \n",
    "# for reference on rasterized catchments DynamicMap\n",
    "tiles_background = gv.tile_sources.CartoLight #OSM\n",
    "points_background = hv.Points(points_gdf, kdims = ['easting','northing'], vdims = ['gage_id'])    \n",
    "\n",
    "# bind catchment geoviews to widgets (pulling CONUS wide, obs data does not warrant selection by HUC2)\n",
    "catchments_bind = pn.bind(\n",
    "    get_historical_chars_geo_element, \n",
    "    data_info = forcing_info,\n",
    "    data_location_id_like_string = \"all\",     \n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,    \n",
    "    variable_name = \"precipitation_flux\",  \n",
    "    geom_gdf = polygons_gdf,\n",
    "    measure = \"sum\",\n",
    "    measure_min_requested = 0,\n",
    "    opts = dict(map_opts)\n",
    ")\n",
    "\n",
    "# bind points dataframe to widgets  - !!! Add option to request list of gages within HUC2   \n",
    "points_bind = pn.bind(\n",
    "    get_historical_chars_geo_element,\n",
    "    data_info = streamflow_info,\n",
    "    data_location_id_like_string = \"all\",     \n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,     \n",
    "    variable_name = \"streamflow\",     \n",
    "    geom_gdf = points_gdf,\n",
    "    measure = flow_measure,\n",
    "    measure_min_requested = 0.1,\n",
    "    opts = dict(map_opts, tools=['hover'])\n",
    ")\n",
    "\n",
    "# Build DynamicMaps - rasterized catchments (left map) and current high flow gage points (right map)\n",
    "aggregator = pn.bind(get_aggregator, \"sum\")\n",
    "raster_catchments = rasterize(hv.DynamicMap(catchments_bind), aggregator=aggregator, precompute=True)\n",
    "points_dmap = hv.DynamicMap(points_bind)\n",
    "\n",
    "# Define stream source as points selection from points_dmap\n",
    "selection_stream = hv.streams.Selection1D(source=points_dmap, index=[0])\n",
    "\n",
    "flow_curve_bind = pn.bind(\n",
    "    get_historical_timeseries_ts_element,\n",
    "    index=selection_stream.param.index,\n",
    "    points_dmap = points_dmap,    \n",
    "    points_info = streamflow_info,\n",
    "    points_gdf = points_gdf,  \n",
    "    variable_name=\"streamflow\",\n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,   \n",
    "    element_type = \"curve\",\n",
    "    opts = dict(curve_opts, height=200)\n",
    ")\n",
    "precip_bars_bind = pn.bind(\n",
    "    get_historical_timeseries_ts_element,\n",
    "    index=selection_stream.param.index,\n",
    "    points_dmap = points_dmap,    \n",
    "    points_info = streamflow_info,\n",
    "    polygons_info = forcing_info,\n",
    "    points_gdf = points_gdf,\n",
    "    polygons_gdf = polygons_gdf, \n",
    "    variable_name = \"precipitation_flux\",\n",
    "    start_value_time = event_dates_slider.param.value_start,\n",
    "    end_value_time = event_dates_slider.param.value_end,\n",
    "    element_type = \"curve\",\n",
    "    opts = dict(curve_opts, xaxis = None, height=150),\n",
    ")   \n",
    "selected_catchment_bind = pn.bind(\n",
    "    get_catchment_polygon_hv,\n",
    "    index=selection_stream.param.index,\n",
    "    points_dmap = points_dmap,    \n",
    "    points_info = streamflow_info,\n",
    "    polygons_info = forcing_info,\n",
    "    points_gdf = points_gdf,\n",
    "    polygons_gdf = polygons_gdf,    \n",
    "    opts = dict(fill_alpha=0, line_color='k', line_width=2),\n",
    ")\n",
    "sel_catch_dmap = hv.DynamicMap(selected_catchment_bind)\n",
    "\n",
    "###### Apply style options that vary by element\n",
    "\n",
    "tiles_background.opts(**map_opts)\n",
    "raster_catchments.opts(**map_opts, colorbar=True, cmap=db1.get_precip_colormap(), clim=(1, 20), toolbar='right', title=precip_map_title)\n",
    "points_background.opts(**map_opts, color='lightgray', size=2, toolbar = 'right')\n",
    "points_dmap.opts(**map_opts, **points_cmap_opts, tools=['hover','tap'], color=hv.dim(flow_measure), \n",
    "                 size=7, legend_position='bottom_right', toolbar='right', title=flow_map_title,\n",
    "                 selection_line_width=5, nonselection_line_width=0, nonselection_alpha=0.5)\n",
    "\n",
    "###### Panel header\n",
    "\n",
    "header = pn.Row(\n",
    "            pn.pane.PNG('https://ciroh.ua.edu/wp-content/uploads/2022/08/CIROHLogo_200x200.png', width=80),\n",
    "            pn.pane.Markdown(\n",
    "                \"\"\"\n",
    "                ## CIROH Tools for Exploratory Evaluation in Hydrology Research (TEEHR)\n",
    "                ## Post-Event - Example 1 - Observed Data Exploration\n",
    "                \"\"\",\n",
    "                width=800\n",
    "            )\n",
    ")\n",
    "# Build the Panel layout\n",
    "layout = \\\n",
    "    pn.Column(\n",
    "        header,\n",
    "        pn.Row(event_dates_slider),\n",
    "        pn.Row((tiles_background * points_background * raster_catchments * sel_catch_dmap) + \n",
    "               (tiles_background * points_background * points_dmap * sel_catch_dmap)),\n",
    "        pn.Row(precip_bars_bind), \n",
    "        pn.Row(flow_curve_bind), \n",
    "        )\n",
    "# launch the layout\n",
    "layout.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163735e-55ad-4579-86fa-16f76ce9a26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
