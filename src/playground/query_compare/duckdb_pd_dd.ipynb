{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09588c10-d327-4b35-84ab-75f07ec6ac7c",
   "metadata": {},
   "source": [
    "# Query Approaches Experiment\n",
    "Experiment to compare the performance of several different ways to query data and compute statistics from populations of forecast and observed data pairs from parquet files.  This includes duckdb, pandas and dask dataframes, as well as a hybrid approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dafb00-2340-4365-82d8-dd88d96716cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# adding project dirs to path so code may be referenced from the notebook\n",
    "import sys\n",
    "sys.path.insert(0, '../../evaluation')\n",
    "sys.path.insert(0, '../../evaluation/queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4336f-8c03-48a3-b457-0fd7614ace81",
   "metadata": {},
   "source": [
    "# DuckDB\n",
    "This approach is a straight DuckDB approach where timeseries are queried and metrics calculated in the SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95233fb-926d-439d-9e3e-ed4e7d90e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58161e95-394c-41d7-a01e-fac39a1763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import queries\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e35079-8642-475d-b3b8-7ae2e4cea320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = queries.calculate_catchment_metrics(\n",
    "    config.MEDIUM_RANGE_FORCING_PARQUET,\n",
    "    config.FORCING_ANALYSIS_ASSIM_PARQUET,\n",
    "    group_by=[\"catchment_id\"],\n",
    "    order_by=[\"observed_average\"],\n",
    "    filters=[\n",
    "        {\n",
    "            \"column\": \"catchment_id\",\n",
    "            \"operator\": \"like\",\n",
    "            \"value\": \"18%\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"reference_time\",\n",
    "            \"operator\": \"=\",\n",
    "            \"value\": \"2022-12-25 00:00:00\"\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "df = duckdb.query(query).to_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2725f-f2cc-4b30-93a6-8c58ee9549a5",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "Using this approach we open the parquet files using pandas and calculate the metrics using pandas groupby and aggregate functionality. We only caculate two simple metrics because even with that the performance was not too good.  More metrics would only make it worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc010c-3092-41fc-8e6f-5466d816d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77b4b1-4333-4112-9dbf-27ae12ea2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load forecast data\n",
    "df_forecast = pd.read_parquet(config.MEDIUM_RANGE_FORCING_PARQUET)\n",
    "df_forecast = df_forecast[\n",
    "        (df_forecast[\"catchment_id\"].str.startswith(\"18\")) & \n",
    "        (df_forecast[\"reference_time\"] == pd.Timestamp(2022,12,25,0,0,0))\n",
    "     ]\n",
    "\n",
    "# load obersved data\n",
    "df_observed = pd.read_parquet(config.FORCING_ANALYSIS_ASSIM_PARQUET)\n",
    "df_observed = df_observed[\n",
    "        (df_observed[\"catchment_id\"].str.startswith(\"18\"))\n",
    "     ]\n",
    "\n",
    "# join forecast and observed\n",
    "df_joined = pd.merge(\n",
    "    df_forecast,\n",
    "    df_observed,\n",
    "    on=[\"catchment_id\",\"value_time\"],\n",
    "    suffixes=[\"_forecast\",\"_observed\"],\n",
    "    how=\"inner\"\n",
    ")[[\"catchment_id\",\"reference_time\",\"value_time\",\"value_forecast\", \"value_observed\"]]\n",
    "\n",
    "# groupby and aggregate\n",
    "df_joined.groupby(\"catchment_id\")[[\"value_forecast\",\"value_observed\"]].agg(\n",
    "        average_forecast = (\"value_forecast\", \"mean\"),\n",
    "        average_observed = (\"value_observed\", \"mean\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8177a34-a84d-4797-9ea8-dd1b0aee9a0f",
   "metadata": {},
   "source": [
    "# Dask\n",
    "This approach is very similar to the Pandas approach but uses a dask dataframe.  Performance is slightly better, but not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbde88-4624-43de-8171-06c5513506f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c4c64-f81a-4aa1-8166-f3427741a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3ea86-63ec-45d6-8fc8-ba1ad081c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load forecast data\n",
    "ddf_forecast = dd.read_parquet(config.MEDIUM_RANGE_FORCING_PARQUET)\n",
    "ddf_forecast = ddf_forecast[\n",
    "        (ddf_forecast[\"catchment_id\"].str.startswith(\"18\")) & \n",
    "        (ddf_forecast[\"reference_time\"] == pd.Timestamp(2022,12,25,0,0,0))\n",
    "     ]\n",
    "\n",
    "# load obersved data\n",
    "ddf_observed = dd.read_parquet(config.FORCING_ANALYSIS_ASSIM_PARQUET)\n",
    "ddf_observed = ddf_observed[\n",
    "        (ddf_observed[\"catchment_id\"].str.startswith(\"18\"))\n",
    "     ]\n",
    "\n",
    "# join forecast and observed\n",
    "ddf_joined = dd.merge(\n",
    "    ddf_forecast,\n",
    "    ddf_observed,\n",
    "    on=[\"catchment_id\",\"value_time\"],\n",
    "    suffixes=[\"_forecast\",\"_observed\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# groupby and aggregate\n",
    "ddf_joined.groupby(\"catchment_id\")[[\"value_forecast\",\"value_observed\"]].agg(\n",
    "        average_forecast = (\"value_forecast\", \"mean\"),\n",
    "        average_observed = (\"value_observed\", \"mean\")\n",
    "    ).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98168778-f216-4b2c-b104-435f9efdf443",
   "metadata": {},
   "source": [
    "# Hybrid\n",
    "The hyrid approach uses DuckDB to query out timeseries pairs and then uses pandas to calculate some statistics.  This approach is likely good for smaller datasets, such as forecasts at a single location where you want to calculate non-standard metrics that are difficult to calculate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4817730-01c3-4428-b1ae-9897fc43a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = queries.get_joined_catchment_timeseries(\n",
    "    config.MEDIUM_RANGE_FORCING_PARQUET,\n",
    "    config.FORCING_ANALYSIS_ASSIM_PARQUET,\n",
    "    filters=[\n",
    "        {\n",
    "            \"column\": \"catchment_id\",\n",
    "            \"operator\": \"==\",\n",
    "            \"value\": \"1801010101\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df = duckdb.query(query).to_df()\n",
    "df.groupby([\"catchment_id\",\"lead_time\"])[[\"forecast_value\",\"observed_value\"]].agg(\n",
    "        average_forecast = (\"forecast_value\", \"mean\"),\n",
    "        average_observed = (\"observed_value\", \"mean\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691c6c5-0327-4eee-adcc-215fb2dd96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = queries.calculate_catchment_metrics(\n",
    "    config.MEDIUM_RANGE_FORCING_PARQUET,\n",
    "    config.FORCING_ANALYSIS_ASSIM_PARQUET,\n",
    "    group_by=[\"catchment_id\",\"lead_time\"],\n",
    "    order_by=[\"catchment_id\",\"lead_time\"],\n",
    "    filters=[\n",
    "         {\n",
    "            \"column\": \"catchment_id\",\n",
    "            \"operator\": \"==\",\n",
    "            \"value\": \"1801010101\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df = duckdb.query(query).to_df()\n",
    "df[[\"catchment_id\",\"lead_time\",\"forecast_average\",\"observed_average\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f13f31-3c46-4c9e-93a7-3d7ce929632a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "DuckDB seems to be the fastest way to query and compute metrics and statistics accross large populations of data. For smaller datasets, say just a few locations, pulling the timeseries out and working in Pandas can work and has the benefit of having the power to Pandas to resample, slica e and dice the data in ways that may be difficult in DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d6012-2dee-47d3-883c-f581aadf3456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
